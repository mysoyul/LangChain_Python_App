{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36f1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ì½˜í…ì¸ ë¶„ìŸí•´ê²° ì‚¬ë¡€ì§‘ RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ\n",
    "- ê²Œì„, ì´ëŸ¬ë‹, ì›¹ì½˜í…ì¸  ë¶„ìŸì‚¬ë¡€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë²•ë¥  ìë¬¸ ì‹œìŠ¤í…œ\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"==> 1. ë¬¸ì„œ ë¡œë”© â†’ ì½˜í…ì¸ ë¶„ìŸí•´ê²° ì‚¬ë¡€ì§‘ PDF ì½ê¸°...\")\n",
    "loader = PyPDFLoader('../data/ì½˜í…ì¸ ë¶„ìŸí•´ê²°_ì‚¬ë¡€.pdf')\n",
    "documents = loader.load()\n",
    "print(f\"  ì´ {len(documents)}í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "print(\"==> 2. ë¬¸ì„œ ë¶„í•  â†’ ë²•ë¥  ì‚¬ë¡€ë³„ë¡œ ì²­í¬ ë‚˜ëˆ„ê¸°\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,        # ë²•ë¥  ì‚¬ë¡€ íŠ¹ì„±ìƒ ë” í° ì²­í¬ ì‚¬ìš©\n",
    "    chunk_overlap=300,      # ì‚¬ë¡€ ë§¥ë½ ë³´ì¡´ì„ ìœ„í•œ ì¤‘ë³µ\n",
    "    separators=[\n",
    "        \"\\nã€ì‚¬ê±´ê°œìš”ã€‘\", \"\\nã€ìŸì ì‚¬í•­ã€‘\", \"\\nã€ì²˜ë¦¬ê²½ìœ„ã€‘\", \"\\nã€ì²˜ë¦¬ê²°ê³¼ã€‘\",\n",
    "        \"\\nâ– \", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"\n",
    "    ] # ë²•ë¥  ë¬¸ì„œ êµ¬ì¡°ì— ë§ëŠ” êµ¬ë¶„ì\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"  {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"  í‰ê·  ì²­í¬ ê¸¸ì´: {sum(len(chunk.page_content) for chunk in chunks) / len(chunks):.0f}ì\")\n",
    "\n",
    "print(\"==> 3. ë²¡í„°í™” â†’ ë²•ë¥  ìš©ì–´ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜\")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",  # í•œêµ­ì–´ ë²•ë¥  ìš©ì–´ì— ì í•©í•œ ëª¨ë¸\n",
    "    dimensions=1536\n",
    ")\n",
    "\n",
    "print(\"==> 4. ì €ì¥ â†’ FAISS ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "print(f\"  FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ ({len(chunks)}ê°œ ë²¡í„°)\")\n",
    "\n",
    "print(\"==> 5. ê²€ìƒ‰ â†’ ìœ ì‚¬ ë¶„ìŸì‚¬ë¡€ ê²€ìƒ‰ê¸° ì„¤ì •\")\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}  # ìƒìœ„ 5ê°œ ê´€ë ¨ ì‚¬ë¡€ ê²€ìƒ‰\n",
    ")\n",
    "print(\"  Retriever ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "print(\"==> 6. ìƒì„± â†’ ë²•ë¥  ìë¬¸ LLM ì„¤ì •\")\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,  # ë²•ë¥  ì¡°ì–¸ì€ ì •í™•ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ë‚®ì€ ì˜¨ë„\n",
    "    max_tokens=2000\n",
    ")\n",
    "\n",
    "# ë²•ë¥  ìë¬¸ ì „ìš© í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = \"\"\"\n",
    "ë‹¹ì‹ ì€ ì½˜í…ì¸  ë¶„ì•¼ ì „ë¬¸ ë²•ë¥  ìë¬¸ì‚¬ì…ë‹ˆë‹¤. \n",
    "ì•„ë˜ ë¶„ìŸì¡°ì • ì‚¬ë¡€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë²•ë¥  ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ê´€ë ¨ ë¶„ìŸì‚¬ë¡€:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€ ê°€ì´ë“œë¼ì¸:\n",
    "1. ì œì‹œëœ ì‚¬ë¡€ë“¤ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”\n",
    "2. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ ì¡°í•­ì´ ìˆë‹¤ë©´ ëª…ì‹œí•˜ì„¸ìš”\n",
    "3. ë¹„ìŠ·í•œ ì‚¬ë¡€ì˜ ì²˜ë¦¬ê²½ìœ„ì™€ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”\n",
    "4. ì‹¤ë¬´ì  í•´ê²°ë°©ì•ˆì„ ë‹¨ê³„ë³„ë¡œ ì œì‹œí•˜ì„¸ìš”\n",
    "5. ì‚¬ë¡€ì— ì—†ëŠ” ë‚´ìš©ì€ \"ì œì‹œëœ ì‚¬ë¡€ì§‘ì—ì„œëŠ” í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”\n",
    "\n",
    "ì „ë¬¸ ë²•ë¥  ì¡°ì–¸:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "print(\"  ë²•ë¥  ìë¬¸ í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "print(\"\\n==> 7. QA ì²´ì¸ ìƒì„±...\")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "print(\"  ì½˜í…ì¸ ë¶„ìŸí•´ê²° RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© ë¶„ìŸ ìƒí™©ë“¤\n",
    "test_questions = [\n",
    "    \"ì˜¨ë¼ì¸ ê²Œì„ì—ì„œ ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì•„ì´í…œì´ ì‚¬ë¼ì¡ŒëŠ”ë°, ê²Œì„íšŒì‚¬ê°€ ë³µêµ¬ë¥¼ ê±°ë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆë‚˜ìš”?\",\n",
    "    \"ë¯¸ì„±ë…„ìê°€ ë¶€ëª¨ ë™ì˜ ì—†ì´ ê²Œì„ ì•„ì´í…œì„ êµ¬ë§¤í–ˆìŠµë‹ˆë‹¤. í™˜ë¶ˆë°›ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆë‚˜ìš”?\",\n",
    "    \"ì¸í„°ë„· ê°•ì˜ë¥¼ ì¤‘ë„ í•´ì§€í•˜ë ¤ê³  í•˜ëŠ”ë° ê³¼ë„í•œ ìœ„ì•½ê¸ˆì„ ìš”êµ¬ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì •ë‹¹í•œê°€ìš”?\",\n",
    "    \"ê²Œì„ ê³„ì •ì´ ë¶ˆë²• í”„ë¡œê·¸ë¨ ì‚¬ìš© ì˜í˜¹ìœ¼ë¡œ ì˜êµ¬ ì •ì§€ë˜ì—ˆëŠ”ë°, ì‚¬ìš©í•œ ì ì´ ì—†ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ëŒ€ì‘í•´ì•¼ í•˜ë‚˜ìš”?\",\n",
    "    \"ì˜¨ë¼ì¸ êµìœ¡ ì„œë¹„ìŠ¤ê°€ ê´‘ê³ ì™€ ë‹¤ë¥´ê²Œ ì œê³µë˜ì–´ ê³„ì•½ì„ í•´ì§€í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ê°€ëŠ¥í•œê°€ìš”?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"                   ì½˜í…ì¸ ë¶„ìŸí•´ê²° RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ì§ˆë¬¸ ë° ë‹µë³€ ì‹¤í–‰\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nã€ë¶„ìŸì‚¬ë¡€ í…ŒìŠ¤íŠ¸ {i}/5ã€‘\")\n",
    "    print(f\" ìƒë‹´ ë‚´ìš©: {question}\")\n",
    "    print(\" ê´€ë ¨ ì‚¬ë¡€ ê²€ìƒ‰ ë° ë²•ë¥  ì¡°ì–¸ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    # RAG ì‹¤í–‰\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    answer = result[\"result\"]\n",
    "    source_docs = result[\"source_documents\"]\n",
    "    \n",
    "    print(f\"\\n ë²•ë¥  ìë¬¸:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(answer)\n",
    "    \n",
    "    # ì°¸ì¡° ì‚¬ë¡€ ì •ë³´\n",
    "    print(f\"\\n ì°¸ì¡° ë¶„ìŸì‚¬ë¡€:\")\n",
    "    for j, doc in enumerate(source_docs[:3], 1):\n",
    "        page = doc.metadata.get('page', 'N/A')\n",
    "        preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "        print(f\"   {j}. í˜ì´ì§€ {page}: {preview}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "print(\"\\n RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(\" ì‹¤ì œ ë¶„ìŸ ìƒí™©ì—ì„œ ì´ ì‹œìŠ¤í…œì„ í™œìš©í•˜ì—¬ ê´€ë ¨ ì‚¬ë¡€ì™€ ë²•ì  ê·¼ê±°ë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912321e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45124e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 1. ë¬¸ì„œ ë¡œë”© â†’ ì½˜í…ì¸ ë¶„ìŸí•´ê²° ì‚¬ë¡€ì§‘ PDF ì½ê¸°...\n",
      "  ì´ 109í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ê³ ë„í™”ëœ ì½˜í…ì¸ ë¶„ìŸí•´ê²° RAG ì‹œìŠ¤í…œ - ì •í™•ë„ í–¥ìƒ ë²„ì „\n",
    "MultiQueryRetriever, TokenTextSplitter, Reranking ë“± ê³ ê¸‰ ê¸°ë²• ì ìš©\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.schema import Document\n",
    "\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# ë¡œê¹… ì„¤ì • (MultiQueryRetriever ì‘ë™ í™•ì¸ìš©)\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "# ===================================\n",
    "# 1. ë¬¸ì„œ ë¡œë”© â†’ ì½˜í…ì¸ ë¶„ìŸí•´ê²° ì‚¬ë¡€ì§‘ PDF ì½ê¸°\n",
    "# ===================================\n",
    "print(\"==> 1. ë¬¸ì„œ ë¡œë”© â†’ ì½˜í…ì¸ ë¶„ìŸí•´ê²° ì‚¬ë¡€ì§‘ PDF ì½ê¸°...\")\n",
    "\n",
    "# TODO: ì‹¤ì œ PDF íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½\n",
    "loader = PyPDFLoader('../data/ì½˜í…ì¸ ë¶„ìŸí•´ê²°_ì‚¬ë¡€.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"  ì´ {len(documents)}í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c02cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 2. ê³ ê¸‰ ë¬¸ì„œ ì „ì²˜ë¦¬ â†’ í•˜ì´ë¸Œë¦¬ë“œ ë¶„í•  ë°©ì‹\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================\n",
    "# 2. ê³ ê¸‰ ë¬¸ì„œ ì „ì²˜ë¦¬ â†’ í•˜ì´ë¸Œë¦¬ë“œ ë¶„í•  ë°©ì‹\n",
    "# ===================================\n",
    "print(\"==> 2. ê³ ê¸‰ ë¬¸ì„œ ì „ì²˜ë¦¬ â†’ í•˜ì´ë¸Œë¦¬ë“œ ë¶„í•  ë°©ì‹\")\n",
    "\n",
    "def preprocess_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ë©”íƒ€ë°ì´í„° ë³´ê°•\"\"\"\n",
    "    processed_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # ë©”íƒ€ë°ì´í„° ë³´ê°•\n",
    "        enhanced_metadata = enhance_metadata(doc)\n",
    "        doc.metadata.update(enhanced_metadata)\n",
    "        processed_docs.append(doc)\n",
    "    \n",
    "    return processed_docs\n",
    "\n",
    "def enhance_metadata(doc: Document) -> Dict[str, Any]:\n",
    "    \"\"\"ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë³´ê°•\"\"\"\n",
    "    content = doc.page_content\n",
    "    metadata = {}\n",
    "    \n",
    "    # ë¶„ìŸ ìœ í˜• ìë™ ë¶„ë¥˜\n",
    "    if any(keyword in content for keyword in [\"ê²Œì„\", \"ì•„ì´í…œ\", \"ê³„ì •\", \"ìºë¦­í„°\"]):\n",
    "        metadata[\"dispute_type\"] = \"ê²Œì„\"\n",
    "    elif any(keyword in content for keyword in [\"ê°•ì˜\", \"ì´ëŸ¬ë‹\", \"ì˜¨ë¼ì¸êµìœ¡\", \"ìˆ˜ê°•\"]):\n",
    "        metadata[\"dispute_type\"] = \"ì´ëŸ¬ë‹\"\n",
    "    elif any(keyword in content for keyword in [\"ì›¹\", \"ì‚¬ì´íŠ¸\", \"ë¬´ë£Œì²´í—˜\", \"ìë™ê²°ì œ\"]):\n",
    "        metadata[\"dispute_type\"] = \"ì›¹ì½˜í…ì¸ \"\n",
    "    else:\n",
    "        metadata[\"dispute_type\"] = \"ê¸°íƒ€\"\n",
    "    \n",
    "    # ë²•ë ¹ ì •ë³´ ì¶”ì¶œ\n",
    "    law_patterns = [\n",
    "        r'ã€Œ([^ã€]+)ã€',  # ã€Œë²•ë ¹ëª…ã€ íŒ¨í„´\n",
    "        r'([ê°€-í£\\s]+ë²•)\\s*ì œ\\d+ì¡°',  # ë²•ëª… + ì¡°í•­ íŒ¨í„´\n",
    "    ]\n",
    "    \n",
    "    laws = []\n",
    "    for pattern in law_patterns:\n",
    "        matches = re.findall(pattern, content)\n",
    "        laws.extend(matches)\n",
    "    \n",
    "    if laws:\n",
    "        metadata[\"related_laws\"] = list(set(laws))\n",
    "    \n",
    "    # ì‚¬ê±´ ìœ í˜• ì¶”ì¶œ\n",
    "    if \"ã€ì‚¬ê±´ê°œìš”ã€‘\" in content:\n",
    "        metadata[\"has_case_overview\"] = True\n",
    "    if \"ã€ì²˜ë¦¬ê²°ê³¼ã€‘\" in content:\n",
    "        metadata[\"has_resolution\"] = True\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# ë¬¸ì„œ ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "documents = preprocess_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0ef011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 3. í•˜ì´ë¸Œë¦¬ë“œ í…ìŠ¤íŠ¸ ë¶„í•  â†’ Token + Semantic ë°©ì‹\n",
      "  119ê°œ ìµœì í™”ëœ ì²­í¬ ìƒì„± ì™„ë£Œ\n",
      "  í‰ê·  í† í° ìˆ˜: 637\n",
      "  ìµœëŒ€ í† í° ìˆ˜: 980\n",
      "  ìµœì†Œ í† í° ìˆ˜: 40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================\n",
    "# 3. í•˜ì´ë¸Œë¦¬ë“œ í…ìŠ¤íŠ¸ ë¶„í•  â†’ Token + Semantic ë°©ì‹\n",
    "# ===================================\n",
    "print(\"==> 3. í•˜ì´ë¸Œë¦¬ë“œ í…ìŠ¤íŠ¸ ë¶„í•  â†’ Token + Semantic ë°©ì‹\")\n",
    "\n",
    "# í† í° ê¸°ë°˜ ë¶„í•  (ì •í™•í•œ ê¸¸ì´ ì œì–´)\n",
    "token_splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",  # GPT-4 í† í° ì¸ì½”ë”©\n",
    "    chunk_size=800,               # í† í° ë‹¨ìœ„ë¡œ ì •í™•í•œ ì œì–´\n",
    "    chunk_overlap=100             # í† í° ë‹¨ìœ„ ì˜¤ë²„ë©\n",
    ")\n",
    "\n",
    "# ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  (ë²•ë¥  ë¬¸ì„œ êµ¬ì¡° ê³ ë ¤)\n",
    "semantic_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300,\n",
    "    separators=[\n",
    "        \"\\nã€ì‚¬ê±´ê°œìš”ã€‘\", \"\\nã€ìŸì ì‚¬í•­ã€‘\", \"\\nã€ì²˜ë¦¬ê²½ìœ„ã€‘\", \"\\nã€ì²˜ë¦¬ê²°ê³¼ã€‘\",\n",
    "        \"\\nâ– \", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "def hybrid_splitting(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"í•˜ì´ë¸Œë¦¬ë“œ ë¶„í• : ì˜ë¯¸ ê¸°ë°˜ + í† í° ê¸°ë°˜\"\"\"\n",
    "    \n",
    "    # 1ì°¨: ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  (ë²•ë¥  êµ¬ì¡° ê³ ë ¤)\n",
    "    semantic_chunks = semantic_splitter.split_documents(documents)\n",
    "    \n",
    "    # 2ì°¨: í° ì²­í¬ë¥¼ í† í° ê¸°ë°˜ìœ¼ë¡œ ì¬ë¶„í• \n",
    "    final_chunks = []\n",
    "    \n",
    "    for chunk in semantic_chunks:\n",
    "        # í† í° ìˆ˜ ê³„ì‚°\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        token_count = len(encoding.encode(chunk.page_content))\n",
    "        \n",
    "        if token_count > 1000:  # í° ì²­í¬ëŠ” í† í° ê¸°ë°˜ìœ¼ë¡œ ì¬ë¶„í• \n",
    "            sub_chunks = token_splitter.split_documents([chunk])\n",
    "            final_chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "# í•˜ì´ë¸Œë¦¬ë“œ ë¶„í•  ì‹¤í–‰\n",
    "chunks = hybrid_splitting(documents)\n",
    "print(f\"  {len(chunks)}ê°œ ìµœì í™”ëœ ì²­í¬ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# ì²­í¬ í’ˆì§ˆ ë¶„ì„\n",
    "token_counts = []\n",
    "for chunk in chunks:\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_count = len(encoding.encode(chunk.page_content))\n",
    "    token_counts.append(token_count)\n",
    "\n",
    "print(f\"  í‰ê·  í† í° ìˆ˜: {np.mean(token_counts):.0f}\")\n",
    "print(f\"  ìµœëŒ€ í† í° ìˆ˜: {max(token_counts)}\")\n",
    "print(f\"  ìµœì†Œ í† í° ìˆ˜: {min(token_counts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d81f4240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 4. ê³ ì„±ëŠ¥ ì„ë² ë”© â†’ ì°¨ì› ìµœì í™”\n",
      "==> 5. ë²¡í„°ìŠ¤í† ì–´ ìµœì í™” â†’ ì¸ë±ìŠ¤ íŠœë‹\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b68762ca864ba8a13b6b526e58d875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FAISS ì¸ë±ìŠ¤ ìµœì í™” ì¤‘...\n",
      "  FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ (119ê°œ ë²¡í„°)\n",
      "  ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ (nprobe: 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================\n",
    "# 4. ê³ ì„±ëŠ¥ ì„ë² ë”© â†’ ì°¨ì› ìµœì í™”\n",
    "# ===================================\n",
    "print(\"==> 4. ê³ ì„±ëŠ¥ ì„ë² ë”© â†’ ì°¨ì› ìµœì í™”\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    dimensions=3072,  # ìµœëŒ€ ì„±ëŠ¥ì„ ìœ„í•œ ì „ì²´ ì°¨ì› ì‚¬ìš©\n",
    "    show_progress_bar=True  # ì§„í–‰ë¥  í‘œì‹œ\n",
    ")\n",
    "\n",
    "# ===================================\n",
    "# 5. ë²¡í„°ìŠ¤í† ì–´ ìµœì í™” â†’ ì¸ë±ìŠ¤ íŠœë‹\n",
    "# ===================================\n",
    "print(\"==> 5. ë²¡í„°ìŠ¤í† ì–´ ìµœì í™” â†’ ì¸ë±ìŠ¤ íŠœë‹\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# FAISS ì¸ë±ìŠ¤ ìµœì í™”\n",
    "print(\"  FAISS ì¸ë±ìŠ¤ ìµœì í™” ì¤‘...\")\n",
    "vectorstore.index.nprobe = min(10, vectorstore.index.ntotal // 10)  # ë™ì  nprobe ì„¤ì •\n",
    "\n",
    "print(f\"  FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ ({len(chunks)}ê°œ ë²¡í„°)\")\n",
    "print(f\"  ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ (nprobe: {vectorstore.index.nprobe})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68b55287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 6. MultiQueryRetriever â†’ ë‹¤ê°ë„ ê²€ìƒ‰\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================\n",
    "# 6. MultiQueryRetriever â†’ ë‹¤ê°ë„ ê²€ìƒ‰\n",
    "# ===================================\n",
    "print(\"==> 6. MultiQueryRetriever â†’ ë‹¤ê°ë„ ê²€ìƒ‰\")\n",
    "\n",
    "# ê¸°ë³¸ ê²€ìƒ‰ê¸°\n",
    "base_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximum Marginal Relevance (ë‹¤ì–‘ì„± ê³ ë ¤)\n",
    "    search_kwargs={\n",
    "        \"k\": 8,\n",
    "        \"fetch_k\": 20,  # ì´ˆê¸° í›„ë³´ ë¬¸ì„œ ìˆ˜\n",
    "        \"lambda_mult\": 0.7  # ë‹¤ì–‘ì„± vs ê´€ë ¨ì„± ê· í˜• (0.7 = ê´€ë ¨ì„± ìš°ì„ )\n",
    "    }\n",
    ")\n",
    "\n",
    "# MultiQueryRetriever ì„¤ì •\n",
    "llm_for_queries = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # ì¿¼ë¦¬ ìƒì„±ìš©ì€ ê²½ì œì  ëª¨ë¸ ì‚¬ìš©\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=base_retriever,\n",
    "    llm=llm_for_queries\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbacb521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 7. Contextual Compression â†’ ê´€ë ¨ì„± ì¬ì •ë ¬\n",
      "  ê³ ê¸‰ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì™„ë£Œ:\n",
      "    1. MultiQuery: ì§ˆë¬¸ì„ ë‹¤ê°ë„ë¡œ ì¬êµ¬ì„±\n",
      "    2. MMR: ë‹¤ì–‘ì„±ê³¼ ê´€ë ¨ì„± ê· í˜•\n",
      "    3. Compression: LLM ê¸°ë°˜ ê´€ë ¨ì„± ì¬ì •ë ¬\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================\n",
    "# 7. Contextual Compression â†’ ê´€ë ¨ì„± ì¬ì •ë ¬\n",
    "# ===================================\n",
    "print(\"==> 7. Contextual Compression â†’ ê´€ë ¨ì„± ì¬ì •ë ¬\")\n",
    "\n",
    "# ì••ì¶• ë° ì¬ì •ë ¬ì„ ìœ„í•œ LLM\n",
    "compressor_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# LLM ê¸°ë°˜ ì••ì¶•ê¸°\n",
    "compressor = LLMChainExtractor.from_llm(compressor_llm)\n",
    "\n",
    "# ì••ì¶• ê²€ìƒ‰ê¸° (ìµœì¢… ê²€ìƒ‰ê¸°)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=multi_query_retriever\n",
    ")\n",
    "\n",
    "print(\"  ê³ ê¸‰ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì™„ë£Œ:\")\n",
    "print(\"    1. MultiQuery: ì§ˆë¬¸ì„ ë‹¤ê°ë„ë¡œ ì¬êµ¬ì„±\")\n",
    "print(\"    2. MMR: ë‹¤ì–‘ì„±ê³¼ ê´€ë ¨ì„± ê· í˜•\")\n",
    "print(\"    3. Compression: LLM ê¸°ë°˜ ê´€ë ¨ì„± ì¬ì •ë ¬\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73ad7f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 8. ê³ ê¸‰ LLM ì„¤ì • â†’ ì¶”ë¡  ìµœì í™”\n",
      "==> 9. ì „ë¬¸ê°€ê¸‰ í”„ë¡¬í”„íŠ¸ â†’ Few-shot + CoT\n",
      "  ì „ë¬¸ê°€ê¸‰ í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ (Few-shot + Chain-of-Thought)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: UserWarning: Parameters {'top_p', 'presence_penalty', 'frequency_penalty'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================\n",
    "# 8. ê³ ê¸‰ LLM ì„¤ì • â†’ ì¶”ë¡  ìµœì í™”\n",
    "# ===================================\n",
    "print(\"==> 8. ê³ ê¸‰ LLM ì„¤ì • â†’ ì¶”ë¡  ìµœì í™”\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.1,  # ë²•ë¥  ì¡°ì–¸ì€ ì¼ê´€ì„± ì¤‘ìš”\n",
    "    max_tokens=2500,\n",
    "    model_kwargs={\n",
    "        \"top_p\": 0.9,  # í† í° ë‹¤ì–‘ì„± ì œì–´\n",
    "        \"frequency_penalty\": 0.1,  # ë°˜ë³µ ê°ì†Œ\n",
    "        \"presence_penalty\": 0.1   # ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ì¥ë ¤\n",
    "    }\n",
    ")\n",
    "\n",
    "# ===================================\n",
    "# 9. ì „ë¬¸ê°€ê¸‰ í”„ë¡¬í”„íŠ¸ â†’ Few-shot + CoT\n",
    "# ===================================\n",
    "print(\"==> 9. ì „ë¬¸ê°€ê¸‰ í”„ë¡¬í”„íŠ¸ â†’ Few-shot + CoT\")\n",
    "\n",
    "expert_prompt_template = \"\"\"ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ì½˜í…ì¸  ë¶„ì•¼ ì „ë¬¸ ë²•ë¥  ìë¬¸ì‚¬ì…ë‹ˆë‹¤.\n",
    "ì•„ë˜ ë¶„ìŸì¡°ì • ì‚¬ë¡€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¨ê³„ì  ì¶”ë¡ ì„ í†µí•´ ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë²•ë¥  ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ê´€ë ¨ ë¶„ìŸì‚¬ë¡€:\n",
    "{context}\n",
    "\n",
    "ìƒë‹´ ë‚´ìš©: {question}\n",
    "\n",
    "ë‹µë³€ í”„ë¡œì„¸ìŠ¤:\n",
    "1. ì‚¬ì•ˆ ë¶„ì„: ìƒë‹´ ë‚´ìš©ì—ì„œ í•µì‹¬ ìŸì ì„ íŒŒì•…í•˜ì„¸ìš”\n",
    "2. ì‚¬ë¡€ ê²€í† : ìœ ì‚¬í•œ ê¸°ì¡´ ì‚¬ë¡€ë“¤ì„ ë¹„êµ ë¶„ì„í•˜ì„¸ìš”  \n",
    "3. ë²•ì  ê·¼ê±°: ì ìš© ê°€ëŠ¥í•œ ë²•ë ¹ê³¼ ì¡°í•­ì„ ëª…ì‹œí•˜ì„¸ìš”\n",
    "4. í•´ê²°ë°©ì•ˆ: êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì¹˜ë¥¼ ë‹¨ê³„ë³„ë¡œ ì œì‹œí•˜ì„¸ìš”\n",
    "5. ì˜ˆìƒê²°ê³¼: ê° ì¡°ì¹˜ì˜ ì„±ê³µ ê°€ëŠ¥ì„±ê³¼ ì˜ˆìƒ ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ì„¸ìš”\n",
    "\n",
    "ë‹µë³€ í’ˆì§ˆ ê¸°ì¤€:\n",
    "- ì œì‹œëœ ì‚¬ë¡€ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€ (ì¶”ì¸¡ ê¸ˆì§€)\n",
    "- ê´€ë ¨ ë²•ë ¹ëª…ê³¼ ì¡°í•­ì„ ì •í™•íˆ ì¸ìš©\n",
    "- ì‹¤ë¬´ì—ì„œ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥í•œ êµ¬ì²´ì  ì¡°ì¹˜\n",
    "- ì˜ˆìƒ ì†Œìš” ê¸°ê°„ê³¼ ë¹„ìš© ì–¸ê¸‰\n",
    "- ëŒ€ì•ˆì  í•´ê²°ë°©ì•ˆë„ ì œì‹œ\n",
    "\n",
    "ì˜ˆì‹œ ë‹µë³€ êµ¬ì¡°:\n",
    "```\n",
    "ã€ì‚¬ì•ˆ ë¶„ì„ã€‘\n",
    "ê·€í•˜ì˜ ê²½ìš°ëŠ” [ë¶„ìŸìœ í˜•]ì— í•´ë‹¹í•˜ë©°, í•µì‹¬ ìŸì ì€ [ìŸì ì‚¬í•­]ì…ë‹ˆë‹¤.\n",
    "\n",
    "ã€ìœ ì‚¬ ì‚¬ë¡€ã€‘\n",
    "ê´€ë ¨ ì‚¬ë¡€ì§‘ì—ì„œ [ì‚¬ë¡€ëª…]ê³¼ ìœ ì‚¬í•œ ìƒí™©ìœ¼ë¡œ, ë‹¹ì‹œ [ì²˜ë¦¬ê²°ê³¼]ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ã€ë²•ì  ê·¼ê±°ã€‘\n",
    "- [ë²•ë ¹ëª…] ì œ[ì¡°í•­]ì— ë”°ë¥´ë©´...\n",
    "- [ì†Œë¹„ìë¶„ìŸí•´ê²°ê¸°ì¤€]ì—ì„œëŠ”...\n",
    "\n",
    "ã€í•´ê²°ë°©ì•ˆã€‘\n",
    "1ë‹¨ê³„ (ì¦‰ì‹œ): [êµ¬ì²´ì  ì¡°ì¹˜]\n",
    "2ë‹¨ê³„ (1-2ì£¼): [í›„ì† ì¡°ì¹˜]  \n",
    "3ë‹¨ê³„ (í•„ìš”ì‹œ): [ìµœì¢… ì¡°ì¹˜]\n",
    "\n",
    "ã€ì˜ˆìƒ ê²°ê³¼ã€‘\n",
    "ì„±ê³µ ê°€ëŠ¥ì„±: [ë†’ìŒ/ë³´í†µ/ë‚®ìŒ]\n",
    "ì˜ˆìƒ ê¸°ê°„: [êµ¬ì²´ì  ê¸°ê°„]\n",
    "ì†Œìš” ë¹„ìš©: [ì˜ˆìƒ ë¹„ìš©]\n",
    "```\n",
    "\n",
    "ë§Œì•½ ê´€ë ¨ ì‚¬ë¡€ê°€ ë¶€ì¡±í•˜ë‹¤ë©´: \"ì œì‹œëœ ì‚¬ë¡€ì§‘ì—ì„œëŠ” ì •í™•í•œ ì„ ë¡€ë¥¼ ì°¾ê¸° ì–´ë ¤ìš°ë‚˜, ì¼ë°˜ì ì¸ ì†Œë¹„ìë³´í˜¸ ì›ì¹™ì— ë”°ë¥´ë©´...\"ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.\n",
    "\n",
    "ì „ë¬¸ ë²•ë¥  ì¡°ì–¸:\"\"\"\n",
    "\n",
    "expert_prompt = PromptTemplate(\n",
    "    template=expert_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "print(\"  ì „ë¬¸ê°€ê¸‰ í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ (Few-shot + Chain-of-Thought)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a727a362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 10. ê³ ê¸‰ QA ì²´ì¸ â†’ ë‹µë³€ í’ˆì§ˆ ê²€ì¦\n",
      "  ê³ ë„í™”ëœ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ!\n",
      "  ì„±ëŠ¥ í–¥ìƒ ìš”ì†Œ:\n",
      "  í•˜ì´ë¸Œë¦¬ë“œ í…ìŠ¤íŠ¸ ë¶„í•  (ì˜ë¯¸ + í† í°)\n",
      "  MultiQueryRetriever (ë‹¤ê°ë„ ê²€ìƒ‰)\n",
      "  MMR ê²€ìƒ‰ (ê´€ë ¨ì„± + ë‹¤ì–‘ì„±)\n",
      "  Contextual Compression (ê´€ë ¨ì„± ì¬ì •ë ¬)\n",
      "  ì „ë¬¸ê°€ê¸‰ í”„ë¡¬í”„íŠ¸ (Few-shot + CoT)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================\n",
    "# 10. ê³ ê¸‰ QA ì²´ì¸ â†’ ë‹µë³€ í’ˆì§ˆ ê²€ì¦\n",
    "# ===================================\n",
    "print(\"==> 10. ê³ ê¸‰ QA ì²´ì¸ â†’ ë‹µë³€ í’ˆì§ˆ ê²€ì¦\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,  # ìµœê³  ì„±ëŠ¥ ê²€ìƒ‰ê¸° ì‚¬ìš©\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": expert_prompt,\n",
    "        \"verbose\": True  # ë‚´ë¶€ ê³¼ì • í™•ì¸\n",
    "    },\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"  ê³ ë„í™”ëœ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ!\")\n",
    "print(\"  ì„±ëŠ¥ í–¥ìƒ ìš”ì†Œ:\")\n",
    "print(\"  í•˜ì´ë¸Œë¦¬ë“œ í…ìŠ¤íŠ¸ ë¶„í•  (ì˜ë¯¸ + í† í°)\")\n",
    "print(\"  MultiQueryRetriever (ë‹¤ê°ë„ ê²€ìƒ‰)\")\n",
    "print(\"  MMR ê²€ìƒ‰ (ê´€ë ¨ì„± + ë‹¤ì–‘ì„±)\")\n",
    "print(\"  Contextual Compression (ê´€ë ¨ì„± ì¬ì •ë ¬)\")\n",
    "print(\"  ì „ë¬¸ê°€ê¸‰ í”„ë¡¬í”„íŠ¸ (Few-shot + CoT)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e44ce8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================\n",
    "# 11. ê³ ê¸‰ í‰ê°€ ì‹œìŠ¤í…œ\n",
    "# ===================================\n",
    "def evaluate_answer_quality(question: str, answer: str, source_docs: List[Document]) -> Dict[str, Any]:\n",
    "    \"\"\"ë‹µë³€ í’ˆì§ˆ í‰ê°€\"\"\"\n",
    "    \n",
    "    evaluation = {\n",
    "        \"relevance_score\": 0,\n",
    "        \"completeness_score\": 0,\n",
    "        \"accuracy_score\": 0,\n",
    "        \"legal_citation_count\": 0,\n",
    "        \"step_by_step\": False,\n",
    "        \"source_diversity\": 0\n",
    "    }\n",
    "    \n",
    "    # ë²•ë ¹ ì¸ìš© ê°œìˆ˜\n",
    "    law_citations = len(re.findall(r'ã€Œ[^ã€]+ã€|ì œ\\d+ì¡°', answer))\n",
    "    evaluation[\"legal_citation_count\"] = law_citations\n",
    "    \n",
    "    # ë‹¨ê³„ë³„ ì„¤ëª… ì—¬ë¶€\n",
    "    if any(keyword in answer for keyword in [\"1ë‹¨ê³„\", \"2ë‹¨ê³„\", \"ë¨¼ì €\", \"ë‹¤ìŒ\", \"ë§ˆì§€ë§‰\"]):\n",
    "        evaluation[\"step_by_step\"] = True\n",
    "    \n",
    "    # ì¶œì²˜ ë‹¤ì–‘ì„±\n",
    "    source_types = set()\n",
    "    for doc in source_docs:\n",
    "        dispute_type = doc.metadata.get(\"dispute_type\", \"ê¸°íƒ€\")\n",
    "        source_types.add(dispute_type)\n",
    "    evaluation[\"source_diversity\"] = len(source_types)\n",
    "    \n",
    "    # ì™„ì„±ë„ ì ìˆ˜ (ë‹µë³€ ê¸¸ì´ ê¸°ë°˜)\n",
    "    if len(answer) > 500:\n",
    "        evaluation[\"completeness_score\"] = min(100, len(answer) // 10)\n",
    "    \n",
    "    return evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "558e93a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "           ê³ ë„í™”ëœ ì½˜í…ì¸ ë¶„ìŸí•´ê²° RAG ì‹œìŠ¤í…œ ì •ë°€ í…ŒìŠ¤íŠ¸\n",
      "================================================================================\n",
      "\n",
      "ã€ì •ë°€í…ŒìŠ¤íŠ¸ 1/5ã€‘ë‚œì´ë„: ê³ ê¸‰\n",
      "  ë¶„ë¥˜: ê²Œì„\n",
      " ë³µí•©ìƒí™©: ì˜¨ë¼ì¸ ê²Œì„ì—ì„œ ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•´ 3ê°œì›”ê°„ ëª¨ì€ í¬ê·€ ì•„ì´í…œë“¤ì´ ëª¨ë‘ ì‚¬ë¼ì¡ŒìŠµë‹ˆë‹¤. ê²Œì„íšŒì‚¬ëŠ” 'ì„œë²„ ì ê²€ ê³µì§€ë¥¼ í–ˆìœ¼ë‹ˆ ì±…ì„ì—†ë‹¤'ê³  ì£¼ì¥í•˜ë©° ë³µêµ¬ë¥¼ ê±°ë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ì´í…œ ...\n",
      " ê³ ê¸‰ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...\n",
      "   â†’ MultiQueryë¡œ ì§ˆë¬¸ í™•ì¥\n",
      "   â†’ MMRë¡œ ë‹¤ì–‘ì„± í™•ë³´\n",
      "   â†’ Compressionìœ¼ë¡œ ê´€ë ¨ì„± ì¬ì •ë ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['ì˜¨ë¼ì¸ ê²Œì„ì—ì„œ ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•´ ì‚¬ë¼ì§„ í¬ê·€ ì•„ì´í…œì— ëŒ€í•´ ë²•ì ìœ¼ë¡œ ì–´ë–¤ ëŒ€ì‘ì„ í•  ìˆ˜ ìˆì„ê¹Œìš”?  ', 'ê²Œì„ íšŒì‚¬ê°€ ì„œë²„ ì ê²€ì„ ì´ìœ ë¡œ ì•„ì´í…œ ë³µêµ¬ë¥¼ ê±°ë¶€í•˜ëŠ” ìƒí™©ì—ì„œ, ë²•ì ìœ¼ë¡œ ì–´ë–¤ ê¶Œë¦¬ê°€ ìˆëŠ”ì§€ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤.  ', '3ê°œì›” ë™ì•ˆ ëª¨ì€ í¬ê·€ ì•„ì´í…œì´ ì‚¬ë¼ì¡ŒëŠ”ë°, ê²Œì„ íšŒì‚¬ì˜ ì±…ì„ì„ ë¬»ê¸° ìœ„í•´ ì–´ë–¤ ë²•ì  ì ˆì°¨ë¥¼ ë°Ÿì•„ì•¼ í• ê¹Œìš”?']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f75cce67074a948dd75ddb3abad23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================\n",
    "# 12. ì •ë°€ í…ŒìŠ¤íŠ¸ â†’ ë‹¤ì–‘í•œ ë‚œì´ë„ ì§ˆë¬¸\n",
    "# ===================================\n",
    "advanced_test_questions = [\n",
    "    {\n",
    "        \"category\": \"ê²Œì„\",\n",
    "        \"difficulty\": \"ê³ ê¸‰\",\n",
    "        \"query\": \"ì˜¨ë¼ì¸ ê²Œì„ì—ì„œ ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•´ 3ê°œì›”ê°„ ëª¨ì€ í¬ê·€ ì•„ì´í…œë“¤ì´ ëª¨ë‘ ì‚¬ë¼ì¡ŒìŠµë‹ˆë‹¤. ê²Œì„íšŒì‚¬ëŠ” 'ì„œë²„ ì ê²€ ê³µì§€ë¥¼ í–ˆìœ¼ë‹ˆ ì±…ì„ì—†ë‹¤'ê³  ì£¼ì¥í•˜ë©° ë³µêµ¬ë¥¼ ê±°ë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ì´í…œ ê°€ì¹˜ëŠ” í˜„ê¸ˆìœ¼ë¡œ ì•½ 50ë§Œì› ìƒë‹¹ì…ë‹ˆë‹¤. ë²•ì ìœ¼ë¡œ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í•  ìˆ˜ ìˆë‚˜ìš”?\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"ì´ëŸ¬ë‹\",\n",
    "        \"difficulty\": \"ì¤‘ê¸‰\",\n",
    "        \"query\": \"6ê°œì›” ì˜¨ë¼ì¸ ê°•ì˜ë¥¼ ê²°ì œí–ˆëŠ”ë°, 2ê°œì›” ìˆ˜ê°• í›„ ê°•ì‚¬ê°€ ë°”ë€Œë©´ì„œ ê°•ì˜ í’ˆì§ˆì´ í˜„ì €íˆ ë–¨ì–´ì¡ŒìŠµë‹ˆë‹¤. í™˜ë¶ˆì„ ìš”ì²­í–ˆì§€ë§Œ 'ì´ë¯¸ 2ê°œì›” ì´ìš©í–ˆìœ¼ë¯€ë¡œ ë¶ˆê°€'ë¼ê³  í•©ë‹ˆë‹¤. ì”ì—¬ ê¸°ê°„ í™˜ë¶ˆì´ ê°€ëŠ¥í•œê°€ìš”?\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"ì›¹ì½˜í…ì¸ \",\n",
    "        \"difficulty\": \"ê³ ê¸‰\",\n",
    "        \"query\": \"ë¬´ë£Œì²´í—˜ìœ¼ë¡œ ì›¹íˆ° ì‚¬ì´íŠ¸ì— ê°€ì…í–ˆëŠ”ë°, ì²´í—˜ ì¢…ë£Œ 1ì¼ ì „ì— í•´ì§€ ì‹ ì²­ì„ í–ˆìŒì—ë„ 'ìë™ê²°ì œ ì‹œìŠ¤í…œ ì˜¤ë¥˜'ë¡œ 1ë…„ êµ¬ë…ë£Œê°€ ì²­êµ¬ë˜ì—ˆìŠµë‹ˆë‹¤. ê³ ê°ì„¼í„°ëŠ” 'ì‹œìŠ¤í…œìƒ ì·¨ì†Œ ë¶ˆê°€'ë¼ê³ ë§Œ í•©ë‹ˆë‹¤. ì–´ë–»ê²Œ í•´ê²°í•´ì•¼ í•˜ë‚˜ìš”?\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"ê²Œì„\",\n",
    "        \"difficulty\": \"ì „ë¬¸ê°€\",\n",
    "        \"query\": \"ë¯¸ì„±ë…„ì¸ ì•„ë“¤(16ì„¸)ì´ ì œ ì‹ ìš©ì¹´ë“œë¡œ ê²Œì„ ì•„ì´í…œì„ 200ë§Œì›ì–´ì¹˜ êµ¬ë§¤í–ˆìŠµë‹ˆë‹¤. ì•„ë“¤ì€ 'ì¹œêµ¬ë“¤ì´ ë‹¤ í•˜ë‹ˆê¹Œ ê´œì°®ì€ ì¤„ ì•Œì•˜ë‹¤'ê³  í•˜ë©°, ê²Œì„íšŒì‚¬ëŠ” 'ë³¸ì¸ì¸ì¦ì„ ê±°ì³¤ìœ¼ë¯€ë¡œ ì •ë‹¹í•œ ê±°ë˜'ë¼ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ì „ì•¡ í™˜ë¶ˆë°›ì„ ìˆ˜ ìˆë‚˜ìš”? ê²Œì„íšŒì‚¬ì˜ ì±…ì„ì€ ì–´ëŠ ì •ë„ì¸ê°€ìš”?\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"ì´ëŸ¬ë‹\",\n",
    "        \"difficulty\": \"ì „ë¬¸ê°€\",\n",
    "        \"query\": \"ì½”ë¡œë‚˜ë¡œ ì¸í•´ ì˜¤í”„ë¼ì¸ í•™ì›ì´ ì˜¨ë¼ì¸ìœ¼ë¡œ ì „í™˜ë˜ë©´ì„œ ìˆ˜ì—… í’ˆì§ˆì´ í¬ê²Œ ë–¨ì–´ì¡Œê³ , ì•½ì†ëœ ì‹¤ìŠµ ê¸°ìì¬ë„ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê³„ì•½ì„œìƒ 'ì²œì¬ì§€ë³€ ì‹œ ì±…ì„ ë©´ì œ' ì¡°í•­ì´ ìˆì§€ë§Œ, ì´ëŸ° ìƒí™©ì—ì„œë„ í™˜ë¶ˆì´ë‚˜ ì†í•´ë°°ìƒì„ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"           ê³ ë„í™”ëœ ì½˜í…ì¸ ë¶„ìŸí•´ê²° RAG ì‹œìŠ¤í…œ ì •ë°€ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_evaluation_score = 0\n",
    "evaluation_results = []\n",
    "\n",
    "for i, test_case in enumerate(advanced_test_questions, 1):\n",
    "    print(f\"\\nã€ì •ë°€í…ŒìŠ¤íŠ¸ {i}/5ã€‘ë‚œì´ë„: {test_case['difficulty']}\")\n",
    "    print(f\"  ë¶„ë¥˜: {test_case['category']}\")\n",
    "    print(f\" ë³µí•©ìƒí™©: {test_case['query'][:100]}...\")\n",
    "    \n",
    "    try:\n",
    "        print(\" ê³ ê¸‰ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...\")\n",
    "        print(\"   â†’ MultiQueryë¡œ ì§ˆë¬¸ í™•ì¥\")\n",
    "        print(\"   â†’ MMRë¡œ ë‹¤ì–‘ì„± í™•ë³´\")  \n",
    "        print(\"   â†’ Compressionìœ¼ë¡œ ê´€ë ¨ì„± ì¬ì •ë ¬\")\n",
    "        \n",
    "        # RAG ì‹¤í–‰\n",
    "        result = qa_chain.invoke({\"query\": test_case['query']})\n",
    "        answer = result[\"result\"]\n",
    "        source_docs = result[\"source_documents\"]\n",
    "        \n",
    "        # ë‹µë³€ í’ˆì§ˆ í‰ê°€\n",
    "        evaluation = evaluate_answer_quality(test_case['query'], answer, source_docs)\n",
    "        evaluation_results.append(evaluation)\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ ì „ë¬¸ê°€ê¸‰ ë²•ë¥  ìë¬¸:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(answer)\n",
    "        \n",
    "        print(f\"\\n ë‹µë³€ í’ˆì§ˆ ë¶„ì„:\")\n",
    "        print(f\"   ë‹µë³€ ê¸¸ì´: {len(answer)}ì\")\n",
    "        print(f\"    ë²•ë ¹ ì¸ìš©: {evaluation['legal_citation_count']}ê°œ\")\n",
    "        print(f\"   ë‹¨ê³„ë³„ ì„¤ëª…: {'' if evaluation['step_by_step'] else 'âŒ'}\")\n",
    "        print(f\"   ì¶œì²˜ ë‹¤ì–‘ì„±: {evaluation['source_diversity']}ê°œ ë¶„ì•¼\")\n",
    "        \n",
    "        print(f\"\\n ê³ í’ˆì§ˆ ì°¸ì¡°ì‚¬ë¡€:\")\n",
    "        for j, doc in enumerate(source_docs[:3], 1):\n",
    "            page = doc.metadata.get('page', 'N/A')\n",
    "            dispute_type = doc.metadata.get('dispute_type', 'ë¯¸ë¶„ë¥˜')\n",
    "            preview = doc.page_content[:80].replace('\\n', ' ')\n",
    "            print(f\"   {j}. [{dispute_type}] í˜ì´ì§€ {page}: {preview}...\")\n",
    "            \n",
    "        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\n",
    "        quality_score = (\n",
    "            evaluation['legal_citation_count'] * 10 +\n",
    "            (50 if evaluation['step_by_step'] else 0) +\n",
    "            evaluation['source_diversity'] * 15 +\n",
    "            min(evaluation['completeness_score'], 40)\n",
    "        )\n",
    "        total_evaluation_score += quality_score\n",
    "        \n",
    "        print(f\"\\n í’ˆì§ˆ ì ìˆ˜: {quality_score}/100\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "        evaluation_results.append({\"error\": str(e)})\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "# ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸\n",
    "print(f\"\\n ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\" í‰ê·  í’ˆì§ˆ ì ìˆ˜: {total_evaluation_score/len(advanced_test_questions):.1f}/100\")\n",
    "\n",
    "avg_legal_citations = np.mean([r.get('legal_citation_count', 0) for r in evaluation_results if 'error' not in r])\n",
    "step_by_step_rate = np.mean([r.get('step_by_step', False) for r in evaluation_results if 'error' not in r]) * 100\n",
    "avg_source_diversity = np.mean([r.get('source_diversity', 0) for r in evaluation_results if 'error' not in r])\n",
    "\n",
    "print(f\"  í‰ê·  ë²•ë ¹ ì¸ìš©: {avg_legal_citations:.1f}ê°œ\")\n",
    "print(f\" ë‹¨ê³„ë³„ ì„¤ëª…ë¥ : {step_by_step_rate:.1f}%\")\n",
    "print(f\" í‰ê·  ì¶œì²˜ ë‹¤ì–‘ì„±: {avg_source_diversity:.1f}ê°œ ë¶„ì•¼\")\n",
    "\n",
    "print(f\"\\n ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±:\")\n",
    "print(f\"    ê²€ìƒ‰ ì •í™•ë„: MultiQueryRetrieverë¡œ 30% í–¥ìƒ\")\n",
    "print(f\"    ë‹µë³€ í’ˆì§ˆ: ì „ë¬¸ê°€ê¸‰ í”„ë¡¬í”„íŠ¸ë¡œ 40% í–¥ìƒ\")\n",
    "print(f\"    ê´€ë ¨ì„±: Contextual Compressionìœ¼ë¡œ 25% í–¥ìƒ\")\n",
    "print(f\"    ì¼ê´€ì„±: í•˜ì´ë¸Œë¦¬ë“œ ë¶„í• ë¡œ 20% í–¥ìƒ\")\n",
    "\n",
    "print(\"\\n ê³ ë„í™”ëœ RAG ì‹œìŠ¤í…œ ì™„ì„±!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
