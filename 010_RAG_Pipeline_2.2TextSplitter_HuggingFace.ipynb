{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1F5lTDp5UPf0",
   "metadata": {
    "id": "1F5lTDp5UPf0"
   },
   "source": [
    "### 1) 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
   "metadata": {
    "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8851eed",
   "metadata": {},
   "source": [
    "#### Hugging Face Tokenizer\n",
    "* Hugging Face 토크나이저를 LangChain의 Text Splitter와 함께 활용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2248589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c30acaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # 설치된 PyTorch 버전 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50719e8",
   "metadata": {},
   "source": [
    "##### 1. Model Hub에서 GPT-2 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18c3408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n",
      "\n",
      "I'm a little bit of a nerd. I'm a big nerd. I'm a\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Hugging Face에서 GPT-2 모델과 토크나이저 다운로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 입력 텍스트 토크나이징\n",
    "input_text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 모델 예측 실행\n",
    "output = model.generate(**inputs)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9c30f",
   "metadata": {},
   "source": [
    "##### 2. 감성 분석 (Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec76496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hugging Face is available to download from Amazon.\\n\\nThis is the final edition of the first book to be released by the company.\\n\\nThe book will be available for pre-order at Amazon.\\n\\nThe book will be available for'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# 50256은 **GPT-2의 종료 토큰(End of Sequence, EOS)**을 의미합니다.\n",
    "# 텍스트의 다양성 조절 temperature=0.7 → 적당한 창의성과 일관성 유지함\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", pad_token_id=50256, temperature=0.7)\n",
    "# GPT-2가 \"Hugging Face is\"라는 문장을 보고, 이를 로맨스 소설과 연결하여 이야기를 만들었습니다.\n",
    "print(generator(\"Hugging Face is\", truncation=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3cf3aa",
   "metadata": {},
   "source": [
    "##### 3. IMDB 영화 리뷰 데이터셋 로드 & 미리보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9553a13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n",
      "{'text': 'Even if you\\'re a huge Sandler fan, please don\\'t bother with this extremely disappointing comedy! I bought this movie for $7.99, assuming it has to be at least halfway decent since my man Sandler is in it and because I assumed some women would get naked (judging by the R-rating and scantily-clad women on the cover). Well, there are quite a few scantily-clad women, but none get naked. I\\'m not sure what point this was in Sandler\\'s career, but I\\'m guessing it was even before his SNL days. I can be wrong. This is like watching one of his home movies. He might look back at a cheesy movie like this and reminisce about the good ol\\' times...but we (the audience) are left to dry. This is hardly a \"movie\"! Sandler does a lot of talking to the camera, and even admits at one point that this is \"no-budget\" movie (that\\'s right, not a low-budget movie, a NO-budget movie). So our job is pretty much to laugh AT the quirky characters. There is no steady plot, it\\'s like an extended sketch comedy show--but a crude and badly written one. That guy who played the nasty comedian was completely annoying and it was implausible in the first place that he would receive such a mass audience. And Sandler finds his comic inspiration by saying the one classic Henny Youngman line \"Take my wife, please\" and the audience is on the floor? I\\'m not even going to TRY to make any logic here. Sure, Sandler\\'s current and recent movies are not known for making a lot of sense (the penguin in \"Billy Madison,\" the midget in \"Happy Gilmore\\'s\" Happy Place) but the comedy works. This is a strictly amateurish work, and even if you\\'re curious about Adam\\'s early days in film--you still won\\'t be interested. You\\'re better off checking out his start on SNL or maybe his underrated role in \"Mixed Nuts.\" Of course, the Sandman is not the only actor wasted in this thankless vehicle. Billy Bob Thornton also makes a short appearance, Billy Zane (\"Titanic\") has a supporting role and the great Burt Young (from the \"Rocky\" movies) has a significant role. <br /><br />This awful comedy will most probably be collecting dust on the 99-cent rental section of your local video store--and rightfully so. <br /><br />My score: 3 (out of 10)', 'label': 0}\n",
      "{'text': 'Awful, simply awful. It proves my theory about \"star power.\" This is supposed to be great TV because the guy who directed (battlestar) Titanica is the same guy who directed this shlop schtock schtick about a chick. B O R I N G.<br /><br />Find something a thousand times more interesting to do - like watch your TV with no picture and no sound. 1/10 (I rated it so high b/c there aren\\'t any negative scores in the IMDb.com rating system.)<br /><br />-Zaphoid<br /><br />PS: My theory about \"star power\" is: the more \"star power\" used in a show, the weaker the show is. (It\\'s called an indirect proportionality: quality 1/\"star power\", less \"sp\" makes for better quality, etc. Another way to look at it is: \"more is less.\")<br /><br />-Z', 'label': 0}\n",
      "{'text': 'God, I was bored out of my head as I watched this pilot. I had been expecting a lot from it, as I\\'m a huge fan of James Cameron (and not just since \"Titanic\", I might add), and his name in the credits I thought would be a guarantee of quality (Then again, he also wrote the leaden Strange Days..). But the thing failed miserably at grabbing my attention at any point of its almost two hours of duration. In all that time, it barely went beyond its two line synopsis, and I would be very hard pressed to try to figure out any kind of coherent plot out of all the mess of strands that went nowhere. On top of that, I don\\'t think the acrobatics outdid even those of any regular \"A-Team\" episode. As for Alba, yes, she is gorgeous, of course, but the fact that she only displays one single facial expression the entire movie (pouty and surly), makes me also get bored of her \"gal wit an attitude\" schtick pretty soon. You can count me out of this one, Mr. Cameron!', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(dataset[\"train\"][0])  # 첫 번째 데이터 샘플 출력\n",
    "\n",
    "# \"Titanic\" 영화에 대한 리뷰 찾기\n",
    "titanic_reviews = [review for review in dataset[\"train\"] if \"Titanic\" in review[\"text\"]]\n",
    "\n",
    "# 상위 3개 리뷰 출력\n",
    "for review in titanic_reviews[:3]:\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f1002",
   "metadata": {},
   "source": [
    "##### 4. 토크나이징 실행 (BERT 기반)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32cbedb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hugging', 'face', 'is', 'awesome', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"Hugging Face is awesome!\")\n",
    "print(tokens)\n",
    "# 출력 예: ['hugging', 'face', 'is', 'awesome', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e9cc58",
   "metadata": {},
   "source": [
    "##### 5. Gradio로 간단한 감성 분석 앱 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0395012",
   "metadata": {},
   "source": [
    "* \"I love this product! It's amazing!\"  → (예상 출력: POSITIVE)\n",
    "* \"This movie was terrible. I hated it.\" → (예상 출력: NEGATIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f730c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# pipeline(\"sentiment-analysis\")는 기본적으로 IMDB 감성 분석 모델을 사용합니다.\n",
    "#classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    return classifier(text)[0][\"label\"]\n",
    "\n",
    "demo = gr.Interface(fn=analyze_sentiment, inputs=\"text\", outputs=\"label\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce575c",
   "metadata": {},
   "source": [
    "#### Hugging Face Tokenizer\n",
    "* Hugging Face 토크나이저를 LangChain의 Text Splitter와 함께 활용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b71acddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 원본 텍스트 미리보기:\n",
      " Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Faceboo\n",
      "\n",
      "총 23개의 청크로 분할됨\n",
      "\n",
      "Chunk 1 (25자):\n",
      "Semantic Search (의미론적 검색)\n",
      "\n",
      "Chunk 2 (157자):\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Chunk 3 (37자):\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "Chunk 4 (176자):\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
      "연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n",
      "\n",
      "Chunk 5 (143자):\n",
      "Embedding (임베딩)\n",
      "\n",
      "정의: 단어나 문장을 벡터 공간에 매핑하여 의미적으로 유사한 것들이 가까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 유사하게 위치함.\n",
      "연관 키워드: 벡터화, 자연어 처리, 딥러닝\n",
      "\n",
      "Token (토큰)\n",
      "\n",
      "\n",
      "첫 번째 청크의 토큰 개수: 23\n",
      "첫 번째 청크의 토큰 리스트: ['Sem', 'antic', 'ĠSearch', 'Ġ(', 'ìĿ', 'ĺ', 'ë', '¯', '¸', 'ë', '¡', 'ł', 'ì', 'ł', 'ģ', 'Ġ', 'ê', '²', 'Ģ', 'ì']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# GPT-2 모델의 토크나이저 로드\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "file_path = \"./data/ai-terminology.txt\"\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "print(\"📄 원본 텍스트 미리보기:\\n\", file_content[:200])\n",
    "\n",
    "# CharacterTextSplitter 설정 (Hugging Face 토크나이저 사용)\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,  # 각 청크 크기 (토큰 기준 아님)\n",
    "    chunk_overlap=50,  # 청크 간 중복 부분\n",
    ")\n",
    "\n",
    "# 텍스트 분할 수행\n",
    "split_texts = text_splitter.split_text(file_content)\n",
    "\n",
    "# 분할된 텍스트 출력\n",
    "print(f\"\\n총 {len(split_texts)}개의 청크로 분할됨\\n\")\n",
    "for i, chunk in enumerate(split_texts[:5]):  # 처음 5개만 출력\n",
    "    print(f\"Chunk {i+1} ({len(chunk)}자):\\n{chunk}\\n\")\n",
    "\n",
    "# 토크나이저로 텍스트를 토큰 단위로 변환하여 확인\n",
    "tokenized_example = hf_tokenizer.tokenize(split_texts[0])\n",
    "print(f\"\\n첫 번째 청크의 토큰 개수: {len(tokenized_example)}\")\n",
    "print(\"첫 번째 청크의 토큰 리스트:\", tokenized_example[:20])  # 앞 20개만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6009baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 원본 텍스트:\n",
      " Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.\n",
      "NLP enables machines to read, understand, and derive meaning from human languages through various techniques such as tokenization, parsing, and sentiment analysis. \n",
      "\n",
      "문자 단위 분할 결과 (총 8개 청크)\n",
      "\n",
      "Chunk 1: Natural Language Processing (NLP) is a subfield of\n",
      "Chunk 2: of artificial intelligence (AI) that focuses on\n",
      "Chunk 3: on the interaction between computers and humans\n",
      "Chunk 4: humans through natural language.\n",
      "Chunk 5: NLP enables machines to read, understand, and\n",
      "\n",
      "토큰 단위 분할 결과 (총 5개 청크)\n",
      "\n",
      "Chunk 1: Natural Language Processing (NLP) is a subfield of artificial intelligence (\n",
      "Chunk 2:  artificial intelligence (AI) that focuses on the interaction between computers and humans through\n",
      "Chunk 3:  and humans through natural language.\n",
      "NLP enables machines to read, understand\n",
      "Chunk 4:  read, understand, and derive meaning from human languages through various techniques such as\n",
      "Chunk 5:  techniques such as tokenization, parsing, and sentiment analysis.\n",
      "\n",
      "토큰 단위 첫 번째 청크의 토큰 개수: 15\n",
      "첫 번째 청크의 토큰 리스트: ['Natural', 'ĠLanguage', 'ĠProcessing', 'Ġ(', 'N', 'LP', ')', 'Ġis', 'Ġa', 'Ġsub', 'field', 'Ġof', 'Ġartificial', 'Ġintelligence', 'Ġ(']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "\n",
    "# ✅ Hugging Face의 GPT-2 토크나이저 로드\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# ✅ 분석할 영어 문장 (한글 파일 대신 직접 입력)\n",
    "text = \"\"\"Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.\n",
    "NLP enables machines to read, understand, and derive meaning from human languages through various techniques such as tokenization, parsing, and sentiment analysis.\"\"\"\n",
    "\n",
    "print(\"📄 원본 텍스트:\\n\", text, \"\\n\")\n",
    "\n",
    "# **문맥을 유지하는 RecursiveCharacterTextSplitter 사용**\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,  # 각 청크 크기 (토큰 기준이 아니라 의미 단위 기준)\n",
    "    chunk_overlap=10,  # 중복된 텍스트 유지 (문맥 연결을 위해)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # 문단 → 문장 → 단어 순서로 나눔\n",
    ")\n",
    "split_texts_char = text_splitter.split_text(text)\n",
    "\n",
    "print(f\"문자 단위 분할 결과 (총 {len(split_texts_char)}개 청크)\\n\")\n",
    "for i, chunk in enumerate(split_texts_char[:5]):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "\n",
    "# **토큰(Token) 단위 분할**\n",
    "text_splitter_token = TokenTextSplitter(\n",
    "    chunk_size=15,  # 각 청크 크기 (토큰 기준)\n",
    "    chunk_overlap=3,  # 중복되는 토큰 수\n",
    ")\n",
    "split_texts_token = text_splitter_token.split_text(text)\n",
    "\n",
    "print(f\"\\n토큰 단위 분할 결과 (총 {len(split_texts_token)}개 청크)\\n\")\n",
    "for i, chunk in enumerate(split_texts_token[:5]):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "\n",
    "# **첫 번째 청크의 토큰화 결과 확인**\n",
    "tokenized_example = hf_tokenizer.tokenize(split_texts_token[0])\n",
    "print(f\"\\n토큰 단위 첫 번째 청크의 토큰 개수: {len(tokenized_example)}\")\n",
    "print(\"첫 번째 청크의 토큰 리스트:\", tokenized_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad89afc",
   "metadata": {},
   "source": [
    "##### 한국어 특화된 모델(KcBERT, KoBERT) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e1ee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 7개 청크로 분할됨\n",
      "\n",
      "Chunk 1: 자연어 처리는 인공지능의\n",
      "\n",
      "Chunk 2: 인공지능의 핵심 기술 중\n",
      "\n",
      "Chunk 3: 중 하나입니다. NLP\n",
      "\n",
      "Chunk 4: NLP 모델은 의미를\n",
      "\n",
      "Chunk 5: 의미를 이해하고 텍스트를\n",
      "\n",
      "Chunk 6: 텍스트를 생성할 수\n",
      "\n",
      "Chunk 7: 수 있습니다.\n",
      "\n",
      "첫 번째 청크의 토큰 개수: 7\n",
      "첫 번째 청크의 토큰 리스트: ['자연', '##어', '처리', '##는', '인공', '##지능', '##의']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 한국어 지원 토크나이저 사용 (kcbert-base)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")  \n",
    "\n",
    "# 예제 문장 (한글 포함)\n",
    "text = \"자연어 처리는 인공지능의 핵심 기술 중 하나입니다. NLP 모델은 의미를 이해하고 텍스트를 생성할 수 있습니다.\"\n",
    "\n",
    "# 적절한 청크 크기 조정 (너무 작지 않게 설정)\n",
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer, \n",
    "    chunk_size=20,  # 더 큰 청크 크기\n",
    "    chunk_overlap=5  # 문맥 유지를 위한 오버랩\n",
    ")\n",
    "\n",
    "# 텍스트 분할\n",
    "split_texts = splitter.split_text(text)\n",
    "\n",
    "# 실행 결과 출력\n",
    "print(f\"총 {len(split_texts)}개 청크로 분할됨\\n\")\n",
    "for i, chunk in enumerate(split_texts):\n",
    "    print(f\"Chunk {i+1}: {chunk}\\n\")\n",
    "\n",
    "# 첫 번째 청크의 토큰화 결과\n",
    "tokens = tokenizer.tokenize(split_texts[0])\n",
    "print(f\"첫 번째 청크의 토큰 개수: {len(tokens)}\")\n",
    "print(\"첫 번째 청크의 토큰 리스트:\", tokens)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
