{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1F5lTDp5UPf0",
   "metadata": {
    "id": "1F5lTDp5UPf0"
   },
   "source": [
    "### 1) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
   "metadata": {
    "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8851eed",
   "metadata": {},
   "source": [
    "#### Hugging Face Tokenizer\n",
    "* Hugging Face í† í¬ë‚˜ì´ì €ë¥¼ LangChainì˜ Text Splitterì™€ í•¨ê»˜ í™œìš© ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2248589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c30acaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # ì„¤ì¹˜ëœ PyTorch ë²„ì „ í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50719e8",
   "metadata": {},
   "source": [
    "##### 1. Model Hubì—ì„œ GPT-2 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18c3408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n",
      "\n",
      "I'm a little bit of a nerd. I'm a big nerd. I'm a\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Hugging Faceì—ì„œ GPT-2 ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# ì…ë ¥ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•\n",
    "input_text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰\n",
    "output = model.generate(**inputs)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9c30f",
   "metadata": {},
   "source": [
    "##### 2. ê°ì„± ë¶„ì„ (Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec76496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hugging Face is available to download from Amazon.\\n\\nThis is the final edition of the first book to be released by the company.\\n\\nThe book will be available for pre-order at Amazon.\\n\\nThe book will be available for'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# 50256ì€ **GPT-2ì˜ ì¢…ë£Œ í† í°(End of Sequence, EOS)**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "# í…ìŠ¤íŠ¸ì˜ ë‹¤ì–‘ì„± ì¡°ì ˆ temperature=0.7 â†’ ì ë‹¹í•œ ì°½ì˜ì„±ê³¼ ì¼ê´€ì„± ìœ ì§€í•¨\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", pad_token_id=50256, temperature=0.7)\n",
    "# GPT-2ê°€ \"Hugging Face is\"ë¼ëŠ” ë¬¸ì¥ì„ ë³´ê³ , ì´ë¥¼ ë¡œë§¨ìŠ¤ ì†Œì„¤ê³¼ ì—°ê²°í•˜ì—¬ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.\n",
    "print(generator(\"Hugging Face is\", truncation=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3cf3aa",
   "metadata": {},
   "source": [
    "##### 3. IMDB ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹ ë¡œë“œ & ë¯¸ë¦¬ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9553a13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n",
      "{'text': 'Even if you\\'re a huge Sandler fan, please don\\'t bother with this extremely disappointing comedy! I bought this movie for $7.99, assuming it has to be at least halfway decent since my man Sandler is in it and because I assumed some women would get naked (judging by the R-rating and scantily-clad women on the cover). Well, there are quite a few scantily-clad women, but none get naked. I\\'m not sure what point this was in Sandler\\'s career, but I\\'m guessing it was even before his SNL days. I can be wrong. This is like watching one of his home movies. He might look back at a cheesy movie like this and reminisce about the good ol\\' times...but we (the audience) are left to dry. This is hardly a \"movie\"! Sandler does a lot of talking to the camera, and even admits at one point that this is \"no-budget\" movie (that\\'s right, not a low-budget movie, a NO-budget movie). So our job is pretty much to laugh AT the quirky characters. There is no steady plot, it\\'s like an extended sketch comedy show--but a crude and badly written one. That guy who played the nasty comedian was completely annoying and it was implausible in the first place that he would receive such a mass audience. And Sandler finds his comic inspiration by saying the one classic Henny Youngman line \"Take my wife, please\" and the audience is on the floor? I\\'m not even going to TRY to make any logic here. Sure, Sandler\\'s current and recent movies are not known for making a lot of sense (the penguin in \"Billy Madison,\" the midget in \"Happy Gilmore\\'s\" Happy Place) but the comedy works. This is a strictly amateurish work, and even if you\\'re curious about Adam\\'s early days in film--you still won\\'t be interested. You\\'re better off checking out his start on SNL or maybe his underrated role in \"Mixed Nuts.\" Of course, the Sandman is not the only actor wasted in this thankless vehicle. Billy Bob Thornton also makes a short appearance, Billy Zane (\"Titanic\") has a supporting role and the great Burt Young (from the \"Rocky\" movies) has a significant role. <br /><br />This awful comedy will most probably be collecting dust on the 99-cent rental section of your local video store--and rightfully so. <br /><br />My score: 3 (out of 10)', 'label': 0}\n",
      "{'text': 'Awful, simply awful. It proves my theory about \"star power.\" This is supposed to be great TV because the guy who directed (battlestar) Titanica is the same guy who directed this shlop schtock schtick about a chick. B O R I N G.<br /><br />Find something a thousand times more interesting to do - like watch your TV with no picture and no sound. 1/10 (I rated it so high b/c there aren\\'t any negative scores in the IMDb.com rating system.)<br /><br />-Zaphoid<br /><br />PS: My theory about \"star power\" is: the more \"star power\" used in a show, the weaker the show is. (It\\'s called an indirect proportionality: quality 1/\"star power\", less \"sp\" makes for better quality, etc. Another way to look at it is: \"more is less.\")<br /><br />-Z', 'label': 0}\n",
      "{'text': 'God, I was bored out of my head as I watched this pilot. I had been expecting a lot from it, as I\\'m a huge fan of James Cameron (and not just since \"Titanic\", I might add), and his name in the credits I thought would be a guarantee of quality (Then again, he also wrote the leaden Strange Days..). But the thing failed miserably at grabbing my attention at any point of its almost two hours of duration. In all that time, it barely went beyond its two line synopsis, and I would be very hard pressed to try to figure out any kind of coherent plot out of all the mess of strands that went nowhere. On top of that, I don\\'t think the acrobatics outdid even those of any regular \"A-Team\" episode. As for Alba, yes, she is gorgeous, of course, but the fact that she only displays one single facial expression the entire movie (pouty and surly), makes me also get bored of her \"gal wit an attitude\" schtick pretty soon. You can count me out of this one, Mr. Cameron!', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(dataset[\"train\"][0])  # ì²« ë²ˆì§¸ ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥\n",
    "\n",
    "# \"Titanic\" ì˜í™”ì— ëŒ€í•œ ë¦¬ë·° ì°¾ê¸°\n",
    "titanic_reviews = [review for review in dataset[\"train\"] if \"Titanic\" in review[\"text\"]]\n",
    "\n",
    "# ìƒìœ„ 3ê°œ ë¦¬ë·° ì¶œë ¥\n",
    "for review in titanic_reviews[:3]:\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f1002",
   "metadata": {},
   "source": [
    "##### 4. í† í¬ë‚˜ì´ì§• ì‹¤í–‰ (BERT ê¸°ë°˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32cbedb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hugging', 'face', 'is', 'awesome', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"Hugging Face is awesome!\")\n",
    "print(tokens)\n",
    "# ì¶œë ¥ ì˜ˆ: ['hugging', 'face', 'is', 'awesome', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e9cc58",
   "metadata": {},
   "source": [
    "##### 5. Gradioë¡œ ê°„ë‹¨í•œ ê°ì„± ë¶„ì„ ì•± ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0395012",
   "metadata": {},
   "source": [
    "* \"I love this product! It's amazing!\"  â†’ (ì˜ˆìƒ ì¶œë ¥: POSITIVE)\n",
    "* \"This movie was terrible. I hated it.\" â†’ (ì˜ˆìƒ ì¶œë ¥: NEGATIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f730c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# pipeline(\"sentiment-analysis\")ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ IMDB ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "#classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    return classifier(text)[0][\"label\"]\n",
    "\n",
    "demo = gr.Interface(fn=analyze_sentiment, inputs=\"text\", outputs=\"label\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce575c",
   "metadata": {},
   "source": [
    "#### Hugging Face Tokenizer\n",
    "* Hugging Face í† í¬ë‚˜ì´ì €ë¥¼ LangChainì˜ Text Splitterì™€ í•¨ê»˜ í™œìš© ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b71acddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°:\n",
      " Semantic Search (ì˜ë¯¸ë¡ ì  ê²€ìƒ‰)\n",
      "\n",
      "ì •ì˜: ì‚¬ìš©ìì˜ ì§ˆì˜ë¥¼ ë‹¨ìˆœí•œ í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•„ë‹ˆë¼ ë¬¸ë§¥ê³¼ ì˜ë¯¸ë¥¼ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” ê²€ìƒ‰ ë°©ì‹.\n",
      "ì˜ˆì‹œ: \"ìš°ì£¼ íƒì‚¬\"ë¥¼ ê²€ìƒ‰í•˜ë©´ \"ì•„í´ë¡œ 11í˜¸\", \"í™”ì„± íƒì‚¬ ë¡œë²„\"ì™€ ê°™ì€ ì—°ê´€ ì •ë³´ê°€ í¬í•¨ëœ ê²°ê³¼ë¥¼ ì œê³µí•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ìì—°ì–´ ì²˜ë¦¬, ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜, ë°ì´í„° ë§ˆì´ë‹\n",
      "\n",
      "FAISS (Faceboo\n",
      "\n",
      "ì´ 23ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨\n",
      "\n",
      "Chunk 1 (25ì):\n",
      "Semantic Search (ì˜ë¯¸ë¡ ì  ê²€ìƒ‰)\n",
      "\n",
      "Chunk 2 (157ì):\n",
      "ì •ì˜: ì‚¬ìš©ìì˜ ì§ˆì˜ë¥¼ ë‹¨ìˆœí•œ í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•„ë‹ˆë¼ ë¬¸ë§¥ê³¼ ì˜ë¯¸ë¥¼ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” ê²€ìƒ‰ ë°©ì‹.\n",
      "ì˜ˆì‹œ: \"ìš°ì£¼ íƒì‚¬\"ë¥¼ ê²€ìƒ‰í•˜ë©´ \"ì•„í´ë¡œ 11í˜¸\", \"í™”ì„± íƒì‚¬ ë¡œë²„\"ì™€ ê°™ì€ ì—°ê´€ ì •ë³´ê°€ í¬í•¨ëœ ê²°ê³¼ë¥¼ ì œê³µí•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ìì—°ì–´ ì²˜ë¦¬, ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜, ë°ì´í„° ë§ˆì´ë‹\n",
      "\n",
      "Chunk 3 (37ì):\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "Chunk 4 (176ì):\n",
      "ì •ì˜: FAISSëŠ” í˜ì´ìŠ¤ë¶ì—ì„œ ê°œë°œí•œ ê³ ì† ìœ ì‚¬ì„± ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, íŠ¹íˆ ëŒ€ê·œëª¨ ë²¡í„° ì§‘í•©ì—ì„œ ìœ ì‚¬ ë²¡í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ì˜ˆì‹œ: ìˆ˜ë°±ë§Œ ê°œì˜ ì´ë¯¸ì§€ ë²¡í„° ì¤‘ì—ì„œ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ë¥¼ ë¹ ë¥´ê²Œ ì°¾ëŠ” ë° FAISSê°€ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "ì—°ê´€í‚¤ì›Œë“œ: ë²¡í„° ê²€ìƒ‰, ë¨¸ì‹ ëŸ¬ë‹, ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”\n",
      "\n",
      "Chunk 5 (143ì):\n",
      "Embedding (ì„ë² ë”©)\n",
      "\n",
      "ì •ì˜: ë‹¨ì–´ë‚˜ ë¬¸ì¥ì„ ë²¡í„° ê³µê°„ì— ë§¤í•‘í•˜ì—¬ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ê²ƒë“¤ì´ ê°€ê¹Œì´ ìœ„ì¹˜í•˜ë„ë¡ í•˜ëŠ” ê¸°ë²•.\n",
      "ì˜ˆì‹œ: \"ê°•ì•„ì§€\"ì™€ \"ê³ ì–‘ì´\"ì˜ ë²¡í„° í‘œí˜„ì´ ìœ ì‚¬í•˜ê²Œ ìœ„ì¹˜í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ë²¡í„°í™”, ìì—°ì–´ ì²˜ë¦¬, ë”¥ëŸ¬ë‹\n",
      "\n",
      "Token (í† í°)\n",
      "\n",
      "\n",
      "ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í° ê°œìˆ˜: 23\n",
      "ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í° ë¦¬ìŠ¤íŠ¸: ['Sem', 'antic', 'Ä Search', 'Ä (', 'Ã¬Ä¿', 'Äº', 'Ã«', 'Â¯', 'Â¸', 'Ã«', 'Â¡', 'Å‚', 'Ã¬', 'Å‚', 'Ä£', 'Ä ', 'Ãª', 'Â²', 'Ä¢', 'Ã¬']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# GPT-2 ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# ë°ì´í„° íŒŒì¼ ì½ê¸°\n",
    "file_path = \"./data/ai-terminology.txt\"\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "print(\"ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°:\\n\", file_content[:200])\n",
    "\n",
    "# CharacterTextSplitter ì„¤ì • (Hugging Face í† í¬ë‚˜ì´ì € ì‚¬ìš©)\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,  # ê° ì²­í¬ í¬ê¸° (í† í° ê¸°ì¤€ ì•„ë‹˜)\n",
    "    chunk_overlap=50,  # ì²­í¬ ê°„ ì¤‘ë³µ ë¶€ë¶„\n",
    ")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í•  ìˆ˜í–‰\n",
    "split_texts = text_splitter.split_text(file_content)\n",
    "\n",
    "# ë¶„í• ëœ í…ìŠ¤íŠ¸ ì¶œë ¥\n",
    "print(f\"\\nì´ {len(split_texts)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨\\n\")\n",
    "for i, chunk in enumerate(split_texts[:5]):  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥\n",
    "    print(f\"Chunk {i+1} ({len(chunk)}ì):\\n{chunk}\\n\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì €ë¡œ í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì—¬ í™•ì¸\n",
    "tokenized_example = hf_tokenizer.tokenize(split_texts[0])\n",
    "print(f\"\\nì²« ë²ˆì§¸ ì²­í¬ì˜ í† í° ê°œìˆ˜: {len(tokenized_example)}\")\n",
    "print(\"ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í° ë¦¬ìŠ¤íŠ¸:\", tokenized_example[:20])  # ì• 20ê°œë§Œ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6009baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸:\n",
      " Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.\n",
      "NLP enables machines to read, understand, and derive meaning from human languages through various techniques such as tokenization, parsing, and sentiment analysis. \n",
      "\n",
      "ë¬¸ì ë‹¨ìœ„ ë¶„í•  ê²°ê³¼ (ì´ 8ê°œ ì²­í¬)\n",
      "\n",
      "Chunk 1: Natural Language Processing (NLP) is a subfield of\n",
      "Chunk 2: of artificial intelligence (AI) that focuses on\n",
      "Chunk 3: on the interaction between computers and humans\n",
      "Chunk 4: humans through natural language.\n",
      "Chunk 5: NLP enables machines to read, understand, and\n",
      "\n",
      "í† í° ë‹¨ìœ„ ë¶„í•  ê²°ê³¼ (ì´ 5ê°œ ì²­í¬)\n",
      "\n",
      "Chunk 1: Natural Language Processing (NLP) is a subfield of artificial intelligence (\n",
      "Chunk 2:  artificial intelligence (AI) that focuses on the interaction between computers and humans through\n",
      "Chunk 3:  and humans through natural language.\n",
      "NLP enables machines to read, understand\n",
      "Chunk 4:  read, understand, and derive meaning from human languages through various techniques such as\n",
      "Chunk 5:  techniques such as tokenization, parsing, and sentiment analysis.\n",
      "\n",
      "í† í° ë‹¨ìœ„ ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í° ê°œìˆ˜: 15\n",
      "ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í° ë¦¬ìŠ¤íŠ¸: ['Natural', 'Ä Language', 'Ä Processing', 'Ä (', 'N', 'LP', ')', 'Ä is', 'Ä a', 'Ä sub', 'field', 'Ä of', 'Ä artificial', 'Ä intelligence', 'Ä (']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "\n",
    "# âœ… Hugging Faceì˜ GPT-2 í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# âœ… ë¶„ì„í•  ì˜ì–´ ë¬¸ì¥ (í•œê¸€ íŒŒì¼ ëŒ€ì‹  ì§ì ‘ ì…ë ¥)\n",
    "text = \"\"\"Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.\n",
    "NLP enables machines to read, understand, and derive meaning from human languages through various techniques such as tokenization, parsing, and sentiment analysis.\"\"\"\n",
    "\n",
    "print(\"ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸:\\n\", text, \"\\n\")\n",
    "\n",
    "# **ë¬¸ë§¥ì„ ìœ ì§€í•˜ëŠ” RecursiveCharacterTextSplitter ì‚¬ìš©**\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,  # ê° ì²­í¬ í¬ê¸° (í† í° ê¸°ì¤€ì´ ì•„ë‹ˆë¼ ì˜ë¯¸ ë‹¨ìœ„ ê¸°ì¤€)\n",
    "    chunk_overlap=10,  # ì¤‘ë³µëœ í…ìŠ¤íŠ¸ ìœ ì§€ (ë¬¸ë§¥ ì—°ê²°ì„ ìœ„í•´)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # ë¬¸ë‹¨ â†’ ë¬¸ì¥ â†’ ë‹¨ì–´ ìˆœì„œë¡œ ë‚˜ëˆ”\n",
    ")\n",
    "split_texts_char = text_splitter.split_text(text)\n",
    "\n",
    "print(f\"ë¬¸ì ë‹¨ìœ„ ë¶„í•  ê²°ê³¼ (ì´ {len(split_texts_char)}ê°œ ì²­í¬)\\n\")\n",
    "for i, chunk in enumerate(split_texts_char[:5]):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "\n",
    "# **í† í°(Token) ë‹¨ìœ„ ë¶„í• **\n",
    "text_splitter_token = TokenTextSplitter(\n",
    "    chunk_size=15,  # ê° ì²­í¬ í¬ê¸° (í† í° ê¸°ì¤€)\n",
    "    chunk_overlap=3,  # ì¤‘ë³µë˜ëŠ” í† í° ìˆ˜\n",
    ")\n",
    "split_texts_token = text_splitter_token.split_text(text)\n",
    "\n",
    "print(f\"\\ní† í° ë‹¨ìœ„ ë¶„í•  ê²°ê³¼ (ì´ {len(split_texts_token)}ê°œ ì²­í¬)\\n\")\n",
    "for i, chunk in enumerate(split_texts_token[:5]):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "\n",
    "# **ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í°í™” ê²°ê³¼ í™•ì¸**\n",
    "tokenized_example = hf_tokenizer.tokenize(split_texts_token[0])\n",
    "print(f\"\\ní† í° ë‹¨ìœ„ ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í° ê°œìˆ˜: {len(tokenized_example)}\")\n",
    "print(\"ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í° ë¦¬ìŠ¤íŠ¸:\", tokenized_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad89afc",
   "metadata": {},
   "source": [
    "##### í•œêµ­ì–´ íŠ¹í™”ëœ ëª¨ë¸(KcBERT, KoBERT) ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e1ee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 7ê°œ ì²­í¬ë¡œ ë¶„í• ë¨\n",
      "\n",
      "Chunk 1: ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜\n",
      "\n",
      "Chunk 2: ì¸ê³µì§€ëŠ¥ì˜ í•µì‹¬ ê¸°ìˆ  ì¤‘\n",
      "\n",
      "Chunk 3: ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. NLP\n",
      "\n",
      "Chunk 4: NLP ëª¨ë¸ì€ ì˜ë¯¸ë¥¼\n",
      "\n",
      "Chunk 5: ì˜ë¯¸ë¥¼ ì´í•´í•˜ê³  í…ìŠ¤íŠ¸ë¥¼\n",
      "\n",
      "Chunk 6: í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜\n",
      "\n",
      "Chunk 7: ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í° ê°œìˆ˜: 7\n",
      "ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í° ë¦¬ìŠ¤íŠ¸: ['ìì—°', '##ì–´', 'ì²˜ë¦¬', '##ëŠ”', 'ì¸ê³µ', '##ì§€ëŠ¥', '##ì˜']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# í•œêµ­ì–´ ì§€ì› í† í¬ë‚˜ì´ì € ì‚¬ìš© (kcbert-base)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")  \n",
    "\n",
    "# ì˜ˆì œ ë¬¸ì¥ (í•œê¸€ í¬í•¨)\n",
    "text = \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•µì‹¬ ê¸°ìˆ  ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. NLP ëª¨ë¸ì€ ì˜ë¯¸ë¥¼ ì´í•´í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# ì ì ˆí•œ ì²­í¬ í¬ê¸° ì¡°ì • (ë„ˆë¬´ ì‘ì§€ ì•Šê²Œ ì„¤ì •)\n",
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer, \n",
    "    chunk_size=20,  # ë” í° ì²­í¬ í¬ê¸°\n",
    "    chunk_overlap=5  # ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•œ ì˜¤ë²„ë©\n",
    ")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• \n",
    "split_texts = splitter.split_text(text)\n",
    "\n",
    "# ì‹¤í–‰ ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ì´ {len(split_texts)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨\\n\")\n",
    "for i, chunk in enumerate(split_texts):\n",
    "    print(f\"Chunk {i+1}: {chunk}\\n\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í°í™” ê²°ê³¼\n",
    "tokens = tokenizer.tokenize(split_texts[0])\n",
    "print(f\"ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í° ê°œìˆ˜: {len(tokens)}\")\n",
    "print(\"ì²« ë²ˆì§¸ ì²­í¬ì˜ í† í° ë¦¬ìŠ¤íŠ¸:\", tokens)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
