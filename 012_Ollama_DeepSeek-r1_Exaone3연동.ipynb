{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U langchain langchain-ollama langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬 Ollama로 설치한 deepseek-r1 모델과 ExaOne3 모델을 사용하기\n",
    "##### ollama run deepseek-r1:7b\n",
    "##### ollama run exaone3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 최신버전 LangChain에서는 ChatOllama와 RunnableSequence(prompt | llm) 를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### deepseeek 모델 9.9 와 9.11 크기 비교문제  (영어로 질문, invoke()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "try:\n",
    "    deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)    # 모델 호출\n",
    "    response = deepseek.invoke(\"which is bigger between 9.9 and 9.11?\")\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### exaone3.4 모델 9.9 와 9.11 크기 비교문제  (한글로 질문, invoke()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "try:\n",
    "    exaone = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5, n_gpu_layers=0, batch_size=128)\n",
    "    #exaone = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "    # 모델 호출\n",
    "    response = exaone.invoke(\"9.9와 9.11 중 무엇이 더 큰가요?\")\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### deepseeek 모델 9.9 와 9.11 크기 비교문제  (영어로 질문, stream()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### deepseeek 모델 9.9 와 9.11 크기 비교문제  (한글로 질문, stream()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exaone 모델 9.9 와 9.11 크기 비교문제  (한글로 질문, stream()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "exaone = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in exaone.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepSeek의 추론 능력과 ExaOne의 한글 생성 능력 결합하기\n",
    "* DeepSeek는 태그 안에서 이루어지는 추론을 기반으로 다른 LLM 대비 높은 성능을 발휘합니다.\n",
    "* 하지만 Ollama에서 제공하는 deepseek r1-distill-qwen 모델은 한국어 생성 능력이 부족합니다.\n",
    "* DeepSeek의 추론 능력과 ExaOne의 한글 생성 능력 결합해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "reasoning_model                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.7)\n",
    "generation_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangGraph 로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#LangGraph에서 State 사용자정의 클래스는 노드 간의 정보를 전달하는 틀입니다. \n",
    "#노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야 할 정보를 미리 정의힙니다. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeepSeek를 통해서 추론 부분까지만 생성합니다. \n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Exaone를 통해서 결과 출력 부분을 생성합니다.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 입력 데이터\n",
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "# invoke()를 사용하여 그래프 호출\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"생성된 답변:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\n",
      "\n",
      "Now, comparing each digit from left to right:\n",
      "\n",
      "- The units place for both numbers is 9.\n",
      "- In the tenths place, 9 has a 9 and 9.11 has a 1.\n",
      "- Since 9 is greater than 1 in the tenths place, 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "추론 결과: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\n",
      "\n",
      "Now, comparing each digit from left to right:\n",
      "\n",
      "- The units place for both numbers is 9.\n",
      "- In the tenths place, 9 has a 9 and 9.11 has a 1.\n",
      "- Since 9 is greater than 1 in the tenths place, 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "\n",
      "물론입니다! 질문에 대한 답변을 드리겠습니다.\n",
      "\n",
      "**답변:**\n",
      "\n",
      "9.9와 9.11을 비교해보면, 두 숫자 모두 정수 부분이 동일한 9입니다. 하지만 소수점 아래의 숫자를 정확히 비교해보면 더 큰 값을 결정할 수 있습니다.\n",
      "\n",
      "먼저, 두 숫자를 소수점 아래 두 자리까지 확장하여 비교해보겠습니다:\n",
      "- 9.9는 그대로 9.9입니다.\n",
      "- 9.11은 소수점 아래 두 자리까지 확장하면 9.11이 됩니다.\n",
      "\n",
      "이제 각 숫자의 소수점 아래 자릿수를 비교해봅시다:\n",
      "- 정수 부분은 모두 같으므로, 소수점 첫 번째 자리를 살펴봅니다. 여기서 9.9의 소수점 첫 번째 자리는 9이고, 9.11의 소수점 첫 번째 자리는 1입니다.\n",
      "- 따라서 9가 1보다 크므로, 9.9가 더 큰 값임을 알 수 있습니다.\n",
      "\n",
      "결론적으로, **9.9가 9.11보다 더 큰 숫자**입니다.생성된 답변: 물론입니다! 질문에 대한 답변을 드리겠습니다.\n",
      "\n",
      "**답변:**\n",
      "\n",
      "9.9와 9.11을 비교해보면, 두 숫자 모두 정수 부분이 동일한 9입니다. 하지만 소수점 아래의 숫자를 정확히 비교해보면 더 큰 값을 결정할 수 있습니다.\n",
      "\n",
      "먼저, 두 숫자를 소수점 아래 두 자리까지 확장하여 비교해보겠습니다:\n",
      "- 9.9는 그대로 9.9입니다.\n",
      "- 9.11은 소수점 아래 두 자리까지 확장하면 9.11이 됩니다.\n",
      "\n",
      "이제 각 숫자의 소수점 아래 자릿수를 비교해봅시다:\n",
      "- 정수 부분은 모두 같으므로, 소수점 첫 번째 자리를 살펴봅니다. 여기서 9.9의 소수점 첫 번째 자리는 9이고, 9.11의 소수점 첫 번째 자리는 1입니다.\n",
      "- 따라서 9가 1보다 크므로, 9.9가 더 큰 값임을 알 수 있습니다.\n",
      "\n",
      "결론적으로, **9.9가 9.11보다 더 큰 숫자**입니다.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추론 결과: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\n",
      "\n",
      "Now, comparing each digit from left to right:\n",
      "\n",
      "- The units place for both numbers is 9.\n",
      "- In the tenths place, 9 has a 9 and 9.11 has a 1.\n",
      "- Since 9 is greater than 1 in the tenths place, 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "\n",
      "생성된 답변: 물론입니다! 질문에 대한 답변 드리겠습니다.\n",
      "\n",
      "**답변:**\n",
      "\n",
      "9.9와 9.11을 비교해보면, 가장 직관적인 방법은 두 숫자를 동일한 소수점 자릿수로 맞추는 것입니다. 이렇게 하면 다음과 같이 비교할 수 있습니다:\n",
      "\n",
      "- **9.9**를 **9.90**으로 표현합니다.\n",
      "- **9.11**은 이미 적절한 형태이지만, 비교를 위해 **9.110**으로 표기해볼 수 있습니다.\n",
      "\n",
      "두 숫자를 비교해보면:\n",
      "\n",
      "- **일의 자리**: 두 숫자 모두 9를 가지고 있습니다.\n",
      "- **십의 자리**: **9.9**의 십의 자리는 9이고, **9.11**의 십의 자리는 1입니다. 따라서 9가 1보다 큽니다.\n",
      "\n",
      "결론적으로, **9.9**가 **9.11**보다 더 큰 수입니다.\n",
      "최종 답변: 물론입니다! 질문에 대한 답변 드리겠습니다.\n",
      "\n",
      "**답변:**\n",
      "\n",
      "9.9와 9.11을 비교해보면, 가장 직관적인 방법은 두 숫자를 동일한 소수점 자릿수로 맞추는 것입니다. 이렇게 하면 다음과 같이 비교할 수 있습니다:\n",
      "\n",
      "- **9.9**를 **9.90**으로 표현합니다.\n",
      "- **9.11**은 이미 적절한 형태이지만, 비교를 위해 **9.110**으로 표기해볼 수 있습니다.\n",
      "\n",
      "두 숫자를 비교해보면:\n",
      "\n",
      "- **일의 자리**: 두 숫자 모두 9를 가지고 있습니다.\n",
      "- **십의 자리**: **9.9**의 십의 자리는 9이고, **9.11**의 십의 자리는 1입니다. 따라서 9가 1보다 큽니다.\n",
      "\n",
      "결론적으로, **9.9**가 **9.11**보다 더 큰 수입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "# 모델 설정: 두 개의 서로 다른 모델을 사용하여 추론과 답변 생성을 수행\n",
    "# - reasoning_model: 추론을 담당하는 모델 (온도 낮음, 정확한 분석용)\n",
    "# - generation_model: 답변 생성을 담당하는 모델 (온도 높음, 창의적 응답용)\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "generation_model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.7)\n",
    "\n",
    "# 상태(State) 정의: 그래프에서 상태를 유지하기 위한 데이터 구조\n",
    "class State(TypedDict):\n",
    "    question: str   # 사용자의 질문\n",
    "    thinking: str   # 추론 결과\n",
    "    answer: str     # 최종 답변\n",
    "\n",
    "# 답변 생성 프롬프트: AI가 답변을 생성할 때 사용할 지침과 입력 형식 정의\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# 추론 함수: 사용자의 질문을 기반으로 AI가 논리적 사고를 수행\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]                    # 입력받은 질문\n",
    "    response = reasoning_model.invoke(question)     # 추론 모델을 사용하여 답변 생성\n",
    "    print(\"추론 결과:\", response.content)            # 추론 결과 출력\n",
    "    return {\"thinking\": response.content}           # 추론 결과를 상태로 반환\n",
    "\n",
    "# 답변 생성 함수: 추론 결과를 바탕으로 AI가 최종적인 답변을 생성\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)    # 답변 생성 모델을 사용하여 최종 응답 생성\n",
    "    print(\"생성된 답변:\", response.content)           # 생성된 답변 출력\n",
    "    return {\"answer\": response.content}             # 생성된 답변을 상태로 반환\n",
    "\n",
    "# 그래프 구성: 상태(State) 간의 흐름을 정의\n",
    "# - think 함수 -> generate 함수 순서로 실행됨\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "\n",
    "# 그래프의 시작 지점을 설정\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "\n",
    "# 그래프 컴파일: 실행 가능한 상태 머신으로 변환\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 입력 데이터: AI가 처리할 질문을 정의\n",
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "# 그래프 실행 및 결과 출력\n",
    "result = graph.invoke(inputs)\n",
    "print(\"최종 답변:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\gradio\\components\\chatbot.py:288: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 2088, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1633, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 850, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\gradio\\chat_interface.py\", line 861, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vega2\\AppData\\Local\\Temp\\ipykernel_77216\\3484142426.py\", line 72, in chatbot_interface\n",
      "    result = graph.invoke(inputs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 2375, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 2031, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\runner.py\", line 230, in tick\n",
      "    run_with_retry(\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\retry.py\", line 40, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 546, in invoke\n",
      "    input = step.invoke(input, config, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 310, in invoke\n",
      "    ret = context.run(self.func, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vega2\\AppData\\Local\\Temp\\ipykernel_77216\\3484142426.py\", line 59, in generate\n",
      "    response = generation_model.invoke(messages)    # 답변 생성 모델을 사용하여 최종 응답 생성\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 307, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 843, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 683, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 908, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_ollama\\chat_models.py\", line 701, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_ollama\\chat_models.py\", line 602, in _chat_stream_with_aggregation\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_ollama\\chat_models.py\", line 589, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\ollama\\_client.py\", line 173, in inner\n",
      "    raise ResponseError(err)\n",
      "ollama._types.ResponseError: POST predict: Post \"http://127.0.0.1:53120/completion\": read tcp 127.0.0.1:53122->127.0.0.1:53120: wsarecv: An existing connection was forcibly closed by the remote host. (status code: -1)\n",
      "During task with name 'generate' and id '5374ffc9-e973-d623-6495-420e2f941a65'\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "# 모델 설정: 두 개의 서로 다른 모델을 사용하여 추론과 답변 생성을 수행\n",
    "# - reasoning_model: 추론을 담당하는 모델 (온도 낮음, 정확한 분석용)\n",
    "# - generation_model: 답변 생성을 담당하는 모델 (온도 높음, 창의적 응답용)\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "generation_model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.7)\n",
    "\n",
    "# 상태(State) 정의: 그래프에서 상태를 유지하기 위한 데이터 구조\n",
    "class State(TypedDict):\n",
    "    question: str   # 사용자의 질문\n",
    "    thinking: str   # 추론 결과\n",
    "    answer: str     # 최종 답변\n",
    "\n",
    "# 답변 생성 프롬프트: AI가 답변을 생성할 때 사용할 지침과 입력 형식 정의\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# 추론 함수: 사용자의 질문을 기반으로 AI가 논리적 사고를 수행\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]                    # 입력받은 질문\n",
    "    response = reasoning_model.invoke(question)     # 추론 모델을 사용하여 답변 생성\n",
    "    return {\"thinking\": response.content}           # 추론 결과를 상태로 반환\n",
    "\n",
    "# 답변 생성 함수: 추론 결과를 바탕으로 AI가 최종적인 답변을 생성\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)    # 답변 생성 모델을 사용하여 최종 응답 생성\n",
    "    return {\"answer\": response.content}             # 생성된 답변을 상태로 반환\n",
    "\n",
    "# 그래프 생성 함수: 상태(State) 간의 흐름을 정의\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio 인터페이스 생성 및 실행\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradio를 사용하여 챗봇 형태로 작성한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 2088, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1635, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 883, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vega2\\AppData\\Local\\Temp\\ipykernel_77216\\4288604849.py\", line 105, in chat\n",
      "    result = graph.invoke(inputs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 2375, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 2031, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\runner.py\", line 230, in tick\n",
      "    run_with_retry(\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\retry.py\", line 40, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 546, in invoke\n",
      "    input = step.invoke(input, config, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 310, in invoke\n",
      "    ret = context.run(self.func, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vega2\\AppData\\Local\\Temp\\ipykernel_77216\\4288604849.py\", line 87, in generate_step\n",
      "    return generate(state, generation_model, answer_prompt)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vega2\\AppData\\Local\\Temp\\ipykernel_77216\\4288604849.py\", line 60, in generate\n",
      "    response = generation_model.invoke(messages)  # 답변 생성 모델을 사용하여 최종 응답 생성\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 307, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 843, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 683, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 908, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_ollama\\chat_models.py\", line 701, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_ollama\\chat_models.py\", line 602, in _chat_stream_with_aggregation\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_ollama\\chat_models.py\", line 589, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"c:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\ollama\\_client.py\", line 173, in inner\n",
      "    raise ResponseError(err)\n",
      "ollama._types.ResponseError: POST predict: Post \"http://127.0.0.1:62303/completion\": read tcp 127.0.0.1:62305->127.0.0.1:62303: wsarecv: An existing connection was forcibly closed by the remote host. (status code: -1)\n",
      "During task with name 'generate' and id '70f57e68-3026-4a6c-17e1-995f28eefe81'\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "# 모델 설정 함수\n",
    "def initialize_models():\n",
    "    reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "    generation_model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.7)\n",
    "    return reasoning_model, generation_model\n",
    "\n",
    "# 상태(State) 정의\n",
    "class State(TypedDict):\n",
    "    question: str  # 사용자의 질문\n",
    "    thinking: str  # 추론 결과\n",
    "    answer: str    # 최종 답변\n",
    "\n",
    "# 답변 생성 프롬프트 설정 함수\n",
    "def create_answer_prompt():\n",
    "    return ChatPromptTemplate([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "            당신의 작업은 다음과 같습니다:\n",
    "            - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "            - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "            - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "            - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "            지침:\n",
    "            - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "            - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "            - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "            - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "            목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"\n",
    "            질문: {question}\n",
    "            추론: {thinking}\n",
    "            \"\"\"\n",
    "        )\n",
    "    ])\n",
    "\n",
    "# 추론 함수\n",
    "def think(state: State, reasoning_model):\n",
    "    question = state[\"question\"]  # 입력받은 질문\n",
    "    response = reasoning_model.invoke(question)  # 추론 모델을 사용하여 답변 생성\n",
    "    return {\"thinking\": response.content}  # 추론 결과를 상태로 반환\n",
    "\n",
    "# 답변 생성 함수\n",
    "def generate(state: State, generation_model, answer_prompt):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)  # 답변 생성 모델을 사용하여 최종 응답 생성\n",
    "    return {\"answer\": response.content}  # 생성된 답변을 상태로 반환\n",
    "\n",
    "# 그래프 생성 함수\n",
    "# def create_graph():\n",
    "#     reasoning_model, generation_model = initialize_models()\n",
    "#     answer_prompt = create_answer_prompt()\n",
    "\n",
    "#     def think_step(state):\n",
    "#         return think(state, reasoning_model)\n",
    "\n",
    "#     def generate_step(state):\n",
    "#         return generate(state, generation_model, answer_prompt)\n",
    "\n",
    "#     graph_builder = StateGraph(State).add_sequence([think_step, generate_step])\n",
    "#     graph_builder.add_edge(START, \"think\")\n",
    "#     return graph_builder.compile()\n",
    "\n",
    "# 그래프 생성 함수\n",
    "def create_graph():\n",
    "    reasoning_model, generation_model = initialize_models()\n",
    "    answer_prompt = create_answer_prompt()\n",
    "\n",
    "    def think_step(state):\n",
    "        return think(state, reasoning_model)\n",
    "\n",
    "    def generate_step(state):\n",
    "        return generate(state, generation_model, answer_prompt)\n",
    "\n",
    "    # 그래프 빌더 생성 및 노드 추가\n",
    "    graph_builder = StateGraph(State)\n",
    "    graph_builder.add_node(\"think\", think_step)\n",
    "    graph_builder.add_node(\"generate\", generate_step)\n",
    "    \n",
    "    # 엣지 추가\n",
    "    graph_builder.set_entry_point(\"think\")\n",
    "    graph_builder.add_edge(\"think\", \"generate\")\n",
    "    graph_builder.set_finish_point(\"generate\")\n",
    "    \n",
    "    return graph_builder.compile()\n",
    "\n",
    "# 실행 함수\n",
    "def chat(question: str):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": question}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "def launch_gradio():\n",
    "    iface = gr.Interface(fn=chat, inputs=\"text\", outputs=\"text\", title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "    iface.launch()\n",
    "\n",
    "# 실행 예시\n",
    "if __name__ == \"__main__\":\n",
    "    launch_gradio()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
