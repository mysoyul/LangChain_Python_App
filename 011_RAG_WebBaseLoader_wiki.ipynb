{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83efc973-52a7-4dd7-a1fd-f0e24729c061",
   "metadata": {
    "id": "83efc973-52a7-4dd7-a1fd-f0e24729c061"
   },
   "source": [
    "#### RAG (Retrieval-Augmented Generation)\n",
    "- 모델의 학습 데이터에 포함되지 않은 데이터를 사용 (환각 방지)\n",
    "- **외부 데이터**를 검색(retrieval)한 후, 생성(generation) 단계에서 LLM에 전달\n",
    "- 모델은 주어진 컨텍스트나 질문에 더 적합하고 풍부한 정보를 반영한 답변을 생성\n",
    "- 논문: https://arxiv.org/abs/2005.11401"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DEJbhYzzUZ-K",
   "metadata": {
    "id": "DEJbhYzzUZ-K"
   },
   "source": [
    "## 0. 환경 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1F5lTDp5UPf0",
   "metadata": {
    "id": "1F5lTDp5UPf0"
   },
   "source": [
    "### 1) 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
   "metadata": {
    "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain langchain-openai langchain_community tiktoken chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "si0unLA0n8Po",
   "metadata": {
    "id": "si0unLA0n8Po"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.21'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55152049-e9e5-4952-8e19-409f58cf3ac9",
   "metadata": {
    "id": "55152049-e9e5-4952-8e19-409f58cf3ac9"
   },
   "source": [
    "### 2) OpenAI 인증키 설정\n",
    "https://openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {
    "id": "b76f68a8-4745-4377-8057-6090b87377d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NU1VnNaZN2-d",
   "metadata": {
    "id": "NU1VnNaZN2-d"
   },
   "source": [
    "## 1. RAG 파이프라인 개요\n",
    "- Load Data - Text Split - Indexing - Retrieval - Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PufJBHgTaA-L",
   "metadata": {
    "id": "PufJBHgTaA-L"
   },
   "source": [
    "##### Step 1: Load Data\n",
    "###### 1. WebBaseLoader를 사용하여 웹 페이지 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa52de-4a17-4c6a-a890-7ef82ed22ffc",
   "metadata": {
    "id": "6f5e3a62-f0a8-4d03-b983-d33de460d229"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# 웹 요청을 위한 USER_AGENT 환경 변수 설정 (필요한 경우)\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "\n",
    "# 환경 변수 확인\n",
    "print(f\"현재 설정된 USER_AGENT: {os.environ.get('USER_AGENT')}\")\n",
    "\n",
    "# 웹페이지 URL 지정  https://ko.wikipedia.org/wiki/축구_경기_규칙\n",
    "url = 'https://ko.wikipedia.org/wiki/%EC%B6%95%EA%B5%AC_%EA%B2%BD%EA%B8%B0_%EA%B7%9C%EC%B9%99'\n",
    "\n",
    "# WebBaseLoader 초기화 및 데이터 로드\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()\n",
    "\n",
    "# 로드된 문서 확인\n",
    "print(type(docs), len(docs))\n",
    "print(docs)\n",
    "print(type(docs[0]))  # <class 'langchain_core.documents.Document'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32e89f-e4f0-48c6-ac62-7d752ea185bf",
   "metadata": {
    "id": "6f5e3a62-f0a8-4d03-b983-d33de460d229"
   },
   "outputs": [],
   "source": [
    "print(len(docs[0].page_content))\n",
    "print(docs[0].page_content[5000:5500])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vzBeboLlaIuy",
   "metadata": {
    "id": "vzBeboLlaIuy"
   },
   "source": [
    "##### Step 2: 문서 분할(Splitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c79cca",
   "metadata": {},
   "source": [
    "* WebBaseLoader를 사용하여 웹 페이지에서 가져온 데이터를 RAG 시스템에서 효율적으로 활용하기 위해 작은 청크(chunks)로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f653d-dcfb-432b-b090-40476aa4c29c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c64f653d-dcfb-432b-b090-40476aa4c29c",
    "outputId": "3ed2f01a-db0a-4256-e4bb-ff998078ba18"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 텍스트 분할기 설정\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "\n",
    "# 문서 분할\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 분할된 문서 확인\n",
    "print(type(splits), len(splits))  # 총 몇 개의 청크가 생성되었는지 확인\n",
    "print(type(splits[0]))\n",
    "print(splits[0].page_content[:20])  # 첫 번째 청크의 일부 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49bd43-5adb-4862-8134-78645ad068e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "bc49bd43-5adb-4862-8134-78645ad068e7",
    "outputId": "71cb4855-b814-4c01-8439-3ffacf576768"
   },
   "outputs": [],
   "source": [
    "# 열번째 청크의 내용 출력\n",
    "print(splits[10].page_content[:20])\n",
    "# 열번째 청크의 메타데이터 출력\n",
    "print(splits[10].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-PLZMd4baaLc",
   "metadata": {
    "id": "-PLZMd4baaLc"
   },
   "source": [
    "##### Step 3: 벡터 DB에 저장 및 검색\n",
    "* Indexing (Texts -> Embedding -> Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b104013-e54c-409f-864e-7e6655a67743",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b104013-e54c-409f-864e-7e6655a67743",
    "outputId": "bd089fd3-981f-41d8-9fe3-f896042318b4"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma  # 벡터 저장소 라이브러리\n",
    "from langchain_openai import OpenAIEmbeddings # OpenAI의 임베딩(Embedding) 모델\n",
    "\n",
    "# 1. Chroma 벡터 저장소 생성\n",
    "# - documents: 텍스트 데이터를 벡터화 하여 저장할 문서 리스트\n",
    "# - embedding: 문서를 벡터로 변환하는 OpenAI Embeddings 모델 사용\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# 2. 유사 문서 검색 (Similarity Search)\n",
    "# - \"경기장 표시에 대해서 설명해주세요.\"라는 쿼리에 대해,\n",
    "# - Chroma 벡터 저장소에서 가장 유사한 문서를 검색함.\n",
    "docs = vectorstore.similarity_search(\"경기장 표시에 대해서 설명해주세요.\")\n",
    "\n",
    "# 3. 검색된 문서의 타입과 개수 출력\n",
    "print(type(docs), len(docs))\n",
    "# 4. 검색된 첫 번째 문서 내용 출력\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mYrHWwsnajPY",
   "metadata": {
    "id": "mYrHWwsnajPY"
   },
   "source": [
    "##### Step 4: RAG Pipeline을 이용한 질의응답 시스템 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99275d3-abb2-4bc3-b783-2afc37d4b666",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "e99275d3-abb2-4bc3-b783-2afc37d4b666",
    "outputId": "39a07730-20ee-43b7-df52-e02a9c70b598"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI  # OpenAI LLM(대화형 언어 모델)\n",
    "from langchain_core.prompts import ChatPromptTemplate  # 프롬프트 템플릿\n",
    "from langchain_core.runnables import RunnablePassthrough  # 입력을 그대로 전달하는 유틸리티\n",
    "from langchain_core.output_parsers import StrOutputParser  # LLM 응답을 문자열로 변환하는 파서\n",
    "\n",
    "# 검색 개수 제한 설정\n",
    "# - 벡터 저장소(vectorstore)에서 관련성이 높은 문서 최대 3개를 검색하도록 설정\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})  \n",
    "\n",
    "# 문서 포맷 변환 함수 (요약 포함)\n",
    "# def format_docs(docs):\n",
    "#     summaries = [doc.page_content[:300] + \"...\" for doc in docs]  \n",
    "#     # - 각 문서의 내용을 앞부분 300자까지만 가져와 요약 (가독성을 위해 \"...\" 추가)\n",
    "#     return \"\\n\\n\".join(summaries)  \n",
    "#     # - 검색된 여러 문서를 하나의 문자열로 합침 (문서 간 개행 추가)\n",
    "\n",
    "def format_docs(docs):\n",
    "    summaries = [f\"출처: {doc.metadata.get('source', '알 수 없음')}\\n\" + doc.page_content[:300] + \"...\" for doc in docs]\n",
    "    return \"\\n\\n\".join(summaries)    \n",
    "\n",
    "# 프롬프트 템플릿 설정\n",
    "# - 모델이 주어진 `context`(검색된 문서)만을 참고하여 질문에 답변하도록 유도하는 프롬프트 템플릿\n",
    "# - {context}: 검색된 문서 요약이 삽입될 자리\n",
    "# - {question}: 사용자의 질문이 삽입될 자리\n",
    "template = '''당신은 제공된 컨텍스트를 기반으로 질문에 답하는 AI 어시스턴트입니다. \n",
    "반드시 컨텍스트 내 정보를 활용하여 정확하고 신뢰할 수 있는 답변을 제공하세요.\n",
    "\n",
    "[컨텍스트]\n",
    "{context}\n",
    "\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "[답변]\n",
    "'''\n",
    "\n",
    "# - 위에서 정의한 템플릿을 사용하여 LangChain의 프롬프트 객체 생성\n",
    "prompt = ChatPromptTemplate.from_template(template)  \n",
    "\n",
    "# LLM 모델 설정\n",
    "# - OpenAI의 `gpt-3.5-turbo-0125` 모델 사용 (정확도를 높이기 위해 temperature=0 설정)\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo-0125', temperature=0)\n",
    "\n",
    "# RAG 체인 설정\n",
    "rag_chain = (\n",
    "    {'context': retriever | format_docs, 'question': RunnablePassthrough()}  \n",
    "    # - `retriever`를 통해 검색된 문서를 `format_docs()` 함수로 가공하여 `context`로 전달\n",
    "    # - `question`은 변형 없이 그대로 전달\n",
    "    | prompt  # - 위에서 정의한 `ChatPromptTemplate`을 적용\n",
    "    | model   # - OpenAI GPT-3.5 모델을 사용해 응답 생성\n",
    "    | StrOutputParser()  # - 모델의 응답을 문자열로 변환\n",
    ")\n",
    "\n",
    "# 실행 (사용자 질문을 입력으로 받아 RAG 체인 실행)\n",
    "# - \"경기장 표시에 대해서 설명해주세요.\"라는 질문을 LLM에 전달하여 답변을 생성\n",
    "response = rag_chain.invoke(\"경기장 표시에 대해서 설명해주세요.\")  \n",
    "\n",
    "# 최종 응답 출력\n",
    "print(f\" 모델 응답:\\n{response}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
