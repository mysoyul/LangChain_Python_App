{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1F5lTDp5UPf0",
   "metadata": {
    "id": "1F5lTDp5UPf0"
   },
   "source": [
    "##### 1) 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
   "metadata": {
    "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain langchain-openai langchain_community tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55152049-e9e5-4952-8e19-409f58cf3ac9",
   "metadata": {
    "id": "55152049-e9e5-4952-8e19-409f58cf3ac9"
   },
   "source": [
    "##### 2) OpenAI 인증키 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {
    "id": "b76f68a8-4745-4377-8057-6090b87377d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb011d",
   "metadata": {},
   "source": [
    "#### CharacterTextSplitter 간단한 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7fe9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['RAG는 검색 기반의 텍스트 생성 모델입니다', '기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다', '특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다', 'RAG는 검색과 생성 단계를 포함합니다']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text = \"\"\"RAG는 검색 기반의 텍스트 생성 모델입니다. 기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다.\n",
    "특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다. \n",
    "RAG는 검색과 생성 단계를 포함합니다.\"\"\"\n",
    "\n",
    "# 마침표(\".\")를 기준으로 텍스트 분할\n",
    "splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=10, separator=\".\")\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "print(type(chunks))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25ae71",
   "metadata": {},
   "source": [
    "#### RecursiveCharacterTextSplitter 간단한 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278a0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Chunk 1: RAG는 검색과 생성 단계를 포함하는 모델입니다.\n",
      "\n",
      "🔹 Chunk 2: 이 모델은 검색 기반의 텍스트 생성 기능을 제공합니다.\n",
      "특히, 최신 데이터를 반영하는 데 강력한 기능을 가지고 있습니다.\n",
      "\n",
      "🔹 Chunk 3: Transformer 모델을 기반으로 실시간 정보를 활용할 수 있으며, 기존의 단순한 생성 모델보다 더 정확한 답변을 제공합니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\"RAG는 검색과 생성 단계를 포함하는 모델입니다.\n",
    "\n",
    "이 모델은 검색 기반의 텍스트 생성 기능을 제공합니다.\n",
    "특히, 최신 데이터를 반영하는 데 강력한 기능을 가지고 있습니다.\n",
    "\n",
    "Transformer 모델을 기반으로 실시간 정보를 활용할 수 있으며, 기존의 단순한 생성 모델보다 더 정확한 답변을 제공합니다.\"\"\"\n",
    "\n",
    "# 의미 단위(문장, 단락)로 나누되, chunk_size 50 제한\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=80, chunk_overlap=20, separators=[\"\\n\\n\", \".\", \"!\", \"?\", \" \", \"\"])\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\" Chunk {i+1}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf721ee",
   "metadata": {},
   "source": [
    "#### TokenTextSplitter 간단한 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3b1d6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텍스트 미리보기:\n",
      " Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
      "연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n",
      "\n",
      "Embedding (임베딩)\n",
      "\n",
      "정의: 단어나 문장을 벡터 공간에 매핑하여 의미적으로 유사한 것들이 가까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 \n",
      "\n",
      "🔹 총 14개의 청크로 분할됨.\n",
      "\n",
      " 첫 번째 청크:\n",
      " Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대�\n",
      "\n",
      "🔹 Chunk 1 (길이: 270):\n",
      "Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대�\n",
      "\n",
      "🔹 Chunk 2 (길이: 229):\n",
      "성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
      "연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n",
      "\n",
      "Embedding (임베딩)\n",
      "\n",
      "정의: 단어나 문장을 벡터 공간에 매핑하여 의미적으로 유사한 것들이 가까이 위치하도록 하는 기법.\n",
      "예시: \"강\n",
      "\n",
      "🔹 Chunk 3 (길이: 233):\n",
      "까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 유사하게 위치함.\n",
      "연관 키워드: 벡터화, 자연어 처리, 딥러닝\n",
      "\n",
      "Token (토큰)\n",
      "\n",
      "정의: 텍스트 데이터를 더 작은 단위(단어, 문자, 문장 등)로 나누는 과정.\n",
      "예시: \"AI는 혁신적이다\"를 [\"AI\", \"는\", \"혁신적\", \"이다\"]로 분할.\n",
      "연관 키워드: 토큰화, NLP, 텍스트 전처리\n",
      "\n",
      "Transformer (트랜스포머)\n",
      "\n",
      "정의: 자\n",
      "\n",
      "🔹 Chunk 4 (길이: 237):\n",
      "�리\n",
      "\n",
      "Transformer (트랜스포머)\n",
      "\n",
      "정의: 자연어 처리에서 사용되는 신경망 아키텍처로, 병렬 연산과 장기 의존성 처리가 강점.\n",
      "예시: GPT, BERT 등의 모델이 트랜스포머 기반으로 동작함.\n",
      "연관 키워드: 딥러닝, 자기 주의 메커니즘, NLP\n",
      "\n",
      "Self-Attention (자기 주의 메커니즘)\n",
      "\n",
      "정의: 문장의 모든 단어가 서로에게 가중치를 부여하여 문맥을 이해하는 방식.\n",
      "예시: \"나는 강아지를 좋아한다\"에서 \"\n",
      "\n",
      "🔹 Chunk 5 (길이: 237):\n",
      ".\n",
      "예시: \"나는 강아지를 좋아한다\"에서 \"나는\"과 \"좋아한다\"가 강한 연관성을 가짐.\n",
      "연관 키워드: 트랜스포머, BERT, 문맥 학습\n",
      "\n",
      "Fine-Tuning (미세 조정)\n",
      "\n",
      "정의: 사전 학습된 모델을 특정 작업에 맞게 추가 학습하는 과정.\n",
      "예시: GPT 모델을 법률 문서 요약에 맞게 학습.\n",
      "연관 키워드: 전이 학습, 모델 최적화, AI 응용\n",
      "\n",
      "Zero-shot Learning (제로샷 학습)\n",
      "\n",
      "정의: 특정 태스크에 대한\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# 파일 읽기\n",
    "with open(\"./data/ai-terminology.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일 내용을 읽어오기\n",
    "\n",
    "print(\"원본 텍스트 미리보기:\\n\", file[:500])  # 앞 500자 출력\n",
    "\n",
    "# TokenTextSplitter 설정\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200,  # 청크 크기\n",
    "    chunk_overlap=20,  # 청크 간 겹치는 부분 추가하여 문맥 유지\n",
    "    encoding_name=\"cl100k_base\",  # OpenAI tiktoken 기본 인코딩 사용 (한글 처리 개선)\n",
    "    add_start_index=True  # 각 청크의 시작 인덱스 반환\n",
    ")\n",
    "\n",
    "# 텍스트 분할 실행\n",
    "texts = text_splitter.split_text(file)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"\\n🔹 총 {len(texts)}개의 청크로 분할됨.\")\n",
    "print(\"\\n 첫 번째 청크:\\n\", texts[0])\n",
    "\n",
    "# 청크 길이 확인\n",
    "for i, chunk in enumerate(texts[:5]):  # 처음 5개만 확인\n",
    "    print(f\"\\n🔹 Chunk {i+1} (길이: {len(chunk)}):\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2248589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6009baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 원본 텍스트 미리보기:\n",
      " Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Faceboo\n",
      "\n",
      "🔹 총 23개의 청크로 분할됨\n",
      "\n",
      "🔹 Chunk 1 (25자):\n",
      "Semantic Search (의미론적 검색)\n",
      "\n",
      "🔹 Chunk 2 (157자):\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "🔹 Chunk 3 (37자):\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "🔹 Chunk 4 (176자):\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
      "연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n",
      "\n",
      "🔹 Chunk 5 (143자):\n",
      "Embedding (임베딩)\n",
      "\n",
      "정의: 단어나 문장을 벡터 공간에 매핑하여 의미적으로 유사한 것들이 가까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 유사하게 위치함.\n",
      "연관 키워드: 벡터화, 자연어 처리, 딥러닝\n",
      "\n",
      "Token (토큰)\n",
      "\n",
      "\n",
      "🔹 첫 번째 청크의 토큰 개수: 23\n",
      "🔹 첫 번째 청크의 토큰 리스트: ['Sem', 'antic', 'ĠSearch', 'Ġ(', 'ìĿ', 'ĺ', 'ë', '¯', '¸', 'ë', '¡', 'ł', 'ì', 'ł', 'ģ', 'Ġ', 'ê', '²', 'Ģ', 'ì']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# GPT-2 모델의 토크나이저 로드\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "file_path = \"./data/ai-terminology.txt\"\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "print(\" 원본 텍스트 미리보기:\\n\", file_content[:200])\n",
    "\n",
    "# CharacterTextSplitter 설정 (Hugging Face 토크나이저 사용)\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,  # 각 청크 크기 (토큰 기준 아님)\n",
    "    chunk_overlap=50,  # 청크 간 중복 부분\n",
    ")\n",
    "\n",
    "# 텍스트 분할 수행\n",
    "split_texts = text_splitter.split_text(file_content)\n",
    "\n",
    "# 분할된 텍스트 출력\n",
    "print(f\"\\n 총 {len(split_texts)}개의 청크로 분할됨\\n\")\n",
    "for i, chunk in enumerate(split_texts[:5]):  # 처음 5개만 출력\n",
    "    print(f\" Chunk {i+1} ({len(chunk)}자):\\n{chunk}\\n\")\n",
    "\n",
    "# 토크나이저로 텍스트를 토큰 단위로 변환하여 확인\n",
    "tokenized_example = hf_tokenizer.tokenize(split_texts[0])\n",
    "print(f\"\\n 첫 번째 청크의 토큰 개수: {len(tokenized_example)}\")\n",
    "print(\" 첫 번째 청크의 토큰 리스트:\", tokenized_example[:20])  # 앞 20개만 출력\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
