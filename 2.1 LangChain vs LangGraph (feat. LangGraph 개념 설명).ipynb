{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.1 LangChain vs LangGraph (feat. LangGraph 개념 설명)\n",
        "\n",
        "- LangChain을 활용한 간단한 `llm.invoke()` 예제를 살펴보고, 이를 LangGraph로 구현해보는 과정을 진행합니다.\n",
        "- LangGraph의 개념과 주요 기능을 이해하고, 두 프레임워크의 차이점을 비교합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 환경설정\n",
        "\n",
        "- `LangChain` 활용을 위해 필요한 패키지들을 설치합니다\n",
        "- 최신 버전을 설치해도 정상적으로 동작해야 하지만, 버전 명시가 필요하다면 `requirements.txt`를 참고해주세요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q python-dotenv langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 먼저 `.env` 파일의 환경변수를 불러옵니다\n",
        "- `OPENAI_API_KEY`, `GEMINI_API_KEY`, `ANTHROPIC_API_KEY` 등과 같이 환경변수를 설정하면 편하게 사용할 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# chain 실행\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m인공지능 모델의 학습 원리에 대하여 쉽게 설명해 주세요.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
            "File \u001b[1;32mc:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:307\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    303\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    304\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    306\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    308\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    309\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    310\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    311\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    312\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    313\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    314\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    315\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    316\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    317\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
            "File \u001b[1;32mc:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:843\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    837\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    841\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    842\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:683\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    682\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 683\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    684\u001b[0m                 m,\n\u001b[0;32m    685\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    686\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    687\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    688\u001b[0m             )\n\u001b[0;32m    689\u001b[0m         )\n\u001b[0;32m    690\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    691\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "File \u001b[1;32mc:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:908\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 908\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    909\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    910\u001b[0m         )\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    912\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:800\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    798\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 800\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
            "File \u001b[1;32mc:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:863\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    860\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    861\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    862\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    866\u001b[0m             {\n\u001b[0;32m    867\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    868\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    869\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    870\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    871\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    872\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    873\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    874\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    875\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    876\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    877\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    878\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    879\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    880\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    881\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    882\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    883\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    884\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    885\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    886\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    887\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    888\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    889\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    890\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    891\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    892\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    893\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    894\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    895\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    896\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    897\u001b[0m             },\n\u001b[0;32m    898\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    899\u001b[0m         ),\n\u001b[0;32m    900\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    901\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    902\u001b[0m         ),\n\u001b[0;32m    903\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    904\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    905\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    906\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1282\u001b[0m     )\n\u001b[1;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
            "File \u001b[1;32mc:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    961\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    962\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    963\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    964\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    965\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    966\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\vega2\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1073\u001b[0m )\n",
            "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# model\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# chain 실행\n",
        "result = llm.invoke(\"인공지능 모델의 학습 원리에 대하여 쉽게 설명해 주세요.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `LangGraph` 활용을 위해 필요한 패키지를 설치합니다\n",
        "- 최신 버전을 설치해도 정상적으로 동작해야 하지만, 버전 명시가 필요하다면 `requirements.txt`를 참고해주세요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `state`는 LangGraph 에이전트의 state를 나타내는 데이터 구조입니다.\n",
        "- `state`는 `TypedDict`를 사용하여 정의되며, 이는 Python의 타입 힌팅을 통해 구조를 명확히 합니다.\n",
        "    - 지금 예제에서는 간단하게 `messages`라는 필드만 있습니다.\n",
        "    - 필요에 따라 다양한 값들을 활용할 수 있습니다.\n",
        "        - 2.2 회차에서 다룰 예정입니다.\n",
        "- `state`는 에이전트의 동작을 결정하는 데 사용되며, 각 노드에서 state를 업데이트하거나 참조할 수 있습니다.\n",
        "- `state`는 LangGraph의 노드 간에 전달되며, 에이전트의 state 전이를 관리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import AnyMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: list[Annotated[AnyMessage, add_messages]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 위에 선언한 `AgentState`를 활용하여 `StateGraph`를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph\n",
        "\n",
        "graph_builder = StateGraph(AgentState)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `graph`에 추가할 `node`를 생성합니다\n",
        "-  `node`는 LangGraph에서 실행되는 개별적인 작업 단위를 의미합니다. \n",
        "    - 각 노드는 특정 기능을 수행하는 독립적인 컴포넌트로, 예를 들어 텍스트 생성, 데이터 처리, 또는 의사 결정과 같은 작업을 담당할 수 있습니다.\n",
        "    - `node`는 기본적으로 함수(function)로 정의되고, 뒤에서 다루지만 다른 에이전트(agent)를 활용할 수도 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    `generate` 노드는 사용자의 질문을 받아서 응답을 생성하는 노드입니다.\n",
        "    \"\"\"\n",
        "    messages = state['messages']\n",
        "    ai_message = llm.invoke(messages)\n",
        "    return {'messages': [ai_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `node`를 생성한 후에 `edge`로 연결합니다\n",
        "- `edge`는 노드들 사이의 연결을 나타내며, 데이터와 제어 흐름의 경로를 정의합니다. \n",
        "    - 엣지를 통해 한 노드의 출력이 다음 노드의 입력으로 전달되어, 전체적인 워크플로우가 형성됩니다.\n",
        "    - `node`와 `edge`의 조합은 방향성 그래프(Directed Graph)를 형성하며, 이를 통해 복잡한 AI 에이전트의 행동 흐름을 구조화할 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x107a6fa40>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_builder.add_node('generate', generate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 모든 그래프는 `START(시작)`와 `END(종료)`가 있습니다\n",
        "    - `END`를 explicit하게 선언하지 않는 경우도 종종 있지만, 가독성을 위해 작성해주는 것을 권장합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x107a6fa40>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langgraph.graph import START, END\n",
        "\n",
        "graph_builder.add_edge(START, 'generate')\n",
        "graph_builder.add_edge('generate', END)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `node`를 생성하고 `edge`로 연결한 후에 `compile` 메서드를 호출하여 `Graph`를 생성합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `compile` 후에는 그래프를 시각화하여 확인할 수 있습니다\n",
        "- 의도한대로 그래프가 생성됐는지 확인하는 습관을 기르는 것이 좋습니다\n",
        "    - `git`에서 코드 작업물을 commit하기 전에 `git diff`를 통해 변경사항을 확인하는 것과 같습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Image\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [AIMessage(content='인프런(Infrun)은 다양한 분야의 온라인 강의를 제공하는 플랫폼으로, 주로 IT와 프로그래밍 관련 강의가 많습니다. 여기에는 다음과 같은 주제가 포함됩니다:\\n\\n1. **프로그래밍 언어**: Python, Java, JavaScript, C++, Ruby 등.\\n2. **웹 개발**: HTML, CSS, React, Vue.js, Node.js 등.\\n3. **모바일 개발**: iOS, Android, Flutter 등.\\n4. **데이터 과학**: 데이터 분석, 머신러닝, 딥러닝, R, Pandas 등.\\n5. **클라우드**: AWS, Azure, Google Cloud 등.\\n6. **DevOps**: Docker, Kubernetes, CI/CD 등.\\n7. **게임 개발**: Unity, Unreal Engine 등.\\n8. **디자인**: UI/UX 디자인, 그래픽 디자인 등.\\n9. **기타**: 프로젝트 관리, IT 자격증 준비 등.\\n\\n각 강의는 강사에 따라 진행 방식이나 내용이 다르므로, 원하는 주제에 맞는 강의를 찾아보시면 좋습니다. 인프런 웹사이트를 방문하면 최신 강의 목록과 강좌에 대한 자세한 정보를 확인할 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 263, 'prompt_tokens': 18, 'total_tokens': 281, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-0f4e5c0f-6e00-4d78-a861-c69dbd71ac7c-0', usage_metadata={'input_tokens': 18, 'output_tokens': 263, 'total_tokens': 281, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "initial_state = {'messages': [HumanMessage(query)]}\n",
        "graph.invoke(initial_state)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
